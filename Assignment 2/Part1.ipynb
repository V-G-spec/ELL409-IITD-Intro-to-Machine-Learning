{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae78186-7827-44d6-bc5d-45080db3b8a1",
   "metadata": {},
   "source": [
    "## Imports and data-reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6074cef-fa8f-4d97-a329-6688f440869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# !pip install cvxopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxopt import matrix, solvers\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a2f15-9e5d-4d03-a593-228e9a32e115",
   "metadata": {},
   "source": [
    "## Useful Functions for any part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c5dc98-0282-4b2d-a7e9-88c494ebd714",
   "metadata": {},
   "source": [
    "### For SVM/SVC plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd494fc1-02a2-46c7-80a4-f4c89cb13301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_it_all_rbf(train_X, train_y, test_X, test_y, reg_pam, ker_coeff_pam):\n",
    "\n",
    "    ######################################### 3D plot ########################################\n",
    "    Z = np.ones((len(reg_pam),len(ker_coeff_pam)))\n",
    "    W = np.ones((len(reg_pam),len(ker_coeff_pam)))\n",
    "    X, Y = np.meshgrid(reg_pam, ker_coeff_pam)\n",
    "\n",
    "\n",
    "    for idxc, c in enumerate(reg_pam):\n",
    "        for idxg, g in enumerate(ker_coeff_pam):\n",
    "            ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='poly', C = c, gamma = g))]\n",
    "            pipeline = Pipeline(ppl) \n",
    "            mod = pipeline\n",
    "#             mod = svm.SVC(kernel='poly', C = c, gamma = g)\n",
    "            mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "            Z[idxc][idxg] = mod.score(train_X, train_y)\n",
    "            W[idxc][idxg] = mod.score(test_X, test_y)\n",
    "\n",
    "    # print(X, Y, Z, W, X.shape, Y.shape, Z.shape, W.shape)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    #ax.plot_wireframe(np.log10(X), np.log10(Y), Z, rstride=1, cstride=1, label='train', color='blue' )\n",
    "    #ax.plot_wireframe(np.log10(X), np.log10(Y), W, rstride=1, cstride=1, label='test', color='red')\n",
    "    #ax.plot_surface(np.log10(X), np.log10(Y), Z, rstride=1, cstride=1, label='train', cmap='viridis', edgecolor='none' )\n",
    "    #ax.plot_surface(np.log10(X), np.log10(Y), W, rstride=1, cstride=1, label='test', cmap='viridis', edgecolor='none')\n",
    "    ax.contour(np.log10(X), np.log10(Y), Z, label='train', cmap='viridis')\n",
    "    ax.contour(np.log10(X), np.log10(Y), W, label='test', cmap='viridis')\n",
    "    ax.colorbar()\n",
    "    ax.set_title('Poly kernel: C and Gamma Vs Accuracy')\n",
    "    ax.set_xlabel('log(C)')\n",
    "    ax.set_ylabel('log(Gamma)')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    ######################################## My conventional plots ########################################\n",
    "    for idxc, c in enumerate(reg_pam):\n",
    "        E_score_train = []\n",
    "        E_score_test = []\n",
    "\n",
    "        for idxg, g in enumerate(ker_coeff_pam):\n",
    "            ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='poly', C = c, gamma = g))]\n",
    "            pipeline = Pipeline(ppl) \n",
    "            mod = pipeline\n",
    "            #mod = svm.SVC(kernel='poly', C = c, gamma = g)\n",
    "            mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "            E_score_train.append(mod.score(train_X, train_y))\n",
    "            E_score_test.append(mod.score(test_X, test_y))\n",
    "\n",
    "        plt.plot(ker_coeff_pam, E_score_test, \"r-\", label=\"Test\")\n",
    "        plt.plot(ker_coeff_pam, E_score_train, \"b-\", label=\"Train\")\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(r'$\\Gamma$')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        plt.title('Score for C=%s'%(c))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "def plot_it_all_linear(train_X, train_y, test_X, test_y, reg_pam):\n",
    "\n",
    "    ######################################## My conventional plots ########################################\n",
    "    E_score_train = []\n",
    "    E_score_test = []\n",
    "\n",
    "    for idxc, c in enumerate(reg_pam):    \n",
    "        ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='linear', C = c))]\n",
    "        pipeline = Pipeline(ppl) \n",
    "        mod = pipeline\n",
    "#         mod = svm.SVC(kernel='poly', C = c, gamma = g)\n",
    "        mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "        E_score_train.append(mod.score(train_X, train_y))\n",
    "        E_score_test.append(mod.score(test_X, test_y))\n",
    "\n",
    "    plt.plot(reg_pam, E_score_test, \"r-\", label=\"Test\")\n",
    "    plt.plot(reg_pam, E_score_train, \"b-\", label=\"Train\")\n",
    "    plt.title(\"Score Vs C for LIBSVM\")\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('C')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "#     plt.title('Score for C=%s'%(c))\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "######################################## PLOTS FOR GRID SEARCH ########################################\n",
    "\n",
    "def plot_search_results(grid):\n",
    "    # Reference: https://stackoverflow.com/questions/37161563/how-to-graph-grid-scores-from-gridsearchcv\n",
    "    \"\"\"\n",
    "    Params: \n",
    "        grid: A trained GridSearchCV object.\n",
    "    \"\"\"\n",
    "    ## Results from grid search\n",
    "    results = grid.cv_results_\n",
    "    means_test = results['mean_test_score']\n",
    "    stds_test = results['std_test_score']\n",
    "    means_train = results['mean_train_score']\n",
    "    stds_train = results['std_train_score']\n",
    "\n",
    "    ## Getting indexes of values per hyper-parameter\n",
    "    masks=[]\n",
    "    masks_names= list(grid.best_params_.keys())\n",
    "    for p_k, p_v in grid.best_params_.items():\n",
    "        masks.append(list(results['param_'+p_k].data==p_v))\n",
    "\n",
    "    params=grid.param_grid\n",
    "\n",
    "    ## Ploting results\n",
    "    fig, ax = plt.subplots(1,len(params),sharex='none', sharey=False,figsize=(len(params)*10,len(params)*2.5))\n",
    "    fig.suptitle('Score per parameter')\n",
    "    fig.text(0.085, 0.5, 'MEAN SCORE', va='center', rotation='vertical')\n",
    "    pram_preformace_in_best = {}\n",
    "    for i, p in enumerate(masks_names):\n",
    "        m = np.stack(masks[:i] + masks[i+1:])\n",
    "#         m=np.array(masks[:i])\n",
    "        pram_preformace_in_best\n",
    "        best_parms_mask = m.all(axis=0)\n",
    "        best_index = np.where(best_parms_mask)[0]\n",
    "        x = np.array(params[p])\n",
    "        y_1 = np.array(means_test[best_index])\n",
    "        e_1 = np.array(stds_test[best_index])\n",
    "        y_2 = np.array(means_train[best_index])\n",
    "        e_2 = np.array(stds_train[best_index])\n",
    "        if (x[-1]>=x[-2]*9):\n",
    "            ax[i].set_xscale('log')\n",
    "        ax[i].errorbar(x, y_1, e_1, linestyle='--', marker='o', label='test')\n",
    "        ax[i].errorbar(x, y_2, e_2, linestyle='-', marker='^',label='train' )\n",
    "        ax[i].set_xlabel(p.upper())\n",
    "        ax[i].grid(True)\n",
    "        ax[i].legend()\n",
    "\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "def plot_search_results_linear(grid, reg_pam):\n",
    "    results = grid.cv_results_\n",
    "    means_test = results['mean_test_score']\n",
    "    stds_test = results['std_test_score']\n",
    "    means_train = results['mean_train_score']\n",
    "    stds_train = results['std_train_score']\n",
    "\n",
    "#     print(\"\")\n",
    "    # print('mean_test_score: ', means_test)\n",
    "    # print('std_test_score: ', stds_test)\n",
    "    # print('mean_train_score: ', means_train)\n",
    "    # print('std_train_score: ', stds_train)\n",
    "    x = reg_pam\n",
    "    y_1 = np.array(means_test)\n",
    "    e_1 = np.array(stds_test)\n",
    "    y_2 = np.array(means_train)\n",
    "    e_2 = np.array(stds_train)\n",
    "    plt.errorbar(x, y_1, e_1, linestyle='--', marker='o', label='test')\n",
    "    plt.errorbar(x, y_2, e_2, linestyle='-', marker='^',label='train' )\n",
    "    plt.grid(True)\n",
    "    plt.xscale('log')\n",
    "    plt.title('Score Vs C for CVXOPT')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('C')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af1107f-483f-4ba8-b0d7-fda77a06b0ab",
   "metadata": {},
   "source": [
    "### For CVXOPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b1931d-8e9a-46d5-9acf-7bee17e42566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://courses.csail.mit.edu/6.867/wiki/images/a/a7/Qp-cvxopt.pdf\n",
    "# Reference: https://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf\n",
    "\n",
    "def cvx_try(X, y, X_test, y_test, which, C=1.0, gamma=0.04, coef0= 0.0, degree=3, threshold=1e-4):\n",
    "    \n",
    "    # Linear: C\n",
    "    # Poly: C, Gamma, Degree, Coef0\n",
    "    # Sigmoid: C, Gamma, Coef0\n",
    "    # RBF: C, gamma\n",
    "    \n",
    "    def makeHP(X1, X2, y_temp, which):\n",
    "        H = np.dot(X1, X2.T)\n",
    "        if (which=='linear'):\n",
    "            P = matrix(y_temp.dot(y_temp.T)*H)\n",
    "        elif (which=='poly'):\n",
    "            H = (gamma*H+coef0)**degree\n",
    "            P = matrix(y_temp.dot(y_temp.T)*H)\n",
    "        elif (which=='sigmoid'):\n",
    "            H = np.tanh(gamma*H+coef0)\n",
    "            P = matrix(y_temp.dot(y_temp.T)*H)\n",
    "        elif (which=='rbf'):\n",
    "            H1 = np.diag(X1.dot(X1.T)).reshape(-1, 1)*np.ones((1, X2.shape[0]))\n",
    "            H2 = np.diag(X2.dot(X2.T)).reshape(1, -1)*np.ones((X1.shape[0],1))\n",
    "            H = 2*H-H1-H2\n",
    "            H = np.exp(gamma*H)\n",
    "            P = matrix(y_temp.dot(y_temp.T)*H)\n",
    "    \n",
    "        return H,P\n",
    "    \n",
    "    y_temp = y.reshape(-1, 1)*1.\n",
    "    H,P = makeHP(X, X, y_temp, which)\n",
    "    q = matrix(-np.ones((X.shape[0], 1)))\n",
    "    A = matrix(y_temp.reshape(1, -1))\n",
    "    b = matrix(np.zeros(1))\n",
    "    G = matrix(np.r_[-1*(np.eye(X.shape[0])), np.eye(X.shape[0])])\n",
    "    h = matrix(np.r_[np.zeros(X.shape[0]), np.ones(X.shape[0])*C])\n",
    "    #print(repr(q),repr(A),repr(b),repr(G),repr(h))\n",
    "    solvers.options['show_progress'] = False\n",
    "    sol = solvers.qp(P,q,G,h,A,b)\n",
    "    lambs, _ = np.array(sol['x']), np.array(sol['primal objective'])\n",
    "    \n",
    "    \n",
    "    ########## GET SUPPORT VECTORS ################\n",
    "    idx = np.where(lambs > threshold)[0] # Indices of support vectors\n",
    "    #Extract support vectors\n",
    "    sX = X[idx,:]\n",
    "    sy = y[idx]\n",
    "    lambs = lambs[idx]\n",
    "    b = np.sum(sy)\n",
    "    for j in idx:\n",
    "        b = b - np.sum(lambs*sy*(H[j, idx].reshape(-1, 1)))\n",
    "    b /= idx.shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######### PREDICT ###########\n",
    "    ynew = np.zeros((X_test.shape[0],))\n",
    "    Htemp,_ = makeHP(sX, test_X, ynew, which)\n",
    "    rightcnt = 0.\n",
    "#     print(ynew.shape, Htemp.shape)\n",
    "    for i in range(ynew.shape[0]):\n",
    "        ynew[i] = np.sum(lambs*sy*Htemp[:,i].reshape(-1,1))+b\n",
    "        if (ynew[i]*y_test[i]>0):\n",
    "            rightcnt+=1.\n",
    "    y_pred = np.sign(ynew)\n",
    "    score = rightcnt*100.0/ynew.shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return lambs, sX, sy, y_pred, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7eb38-91e1-4a7c-b926-285c2785479a",
   "metadata": {},
   "source": [
    "# Part 1A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfee0a-0780-4513-a87a-0e41e4f5e6bf",
   "metadata": {},
   "source": [
    "## CVXOPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428bd38-316c-4ef0-a670-760148f7d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cvx_try(X, y, X_test, y_test, which, C=1.0, gamma=0.04, coef0= 0.0, degree=3, threshold=1e-4):\n",
    "# legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which='poly', C=0.01, gamma=1000, degree=1)\n",
    "\n",
    "\n",
    "# # threshold = 1e-4\n",
    "# # ############ https://xavierbourretsicotte.github.io/SVM_implementation.html #########\n",
    "# # #w parameter in vectorized form\n",
    "# # w = ((train_y * idk).T @ train_X).reshape(-1,1)\n",
    "# # #Selecting the set of indices S corresponding to non zero parameters\n",
    "# # S = (idk > threshold).flatten()\n",
    "# # #Computing b\n",
    "# # b = train_y[S] - np.dot(train_X[S], w)\n",
    "# # #Display results\n",
    "# # # print('Alphas = ',idk[idk > threshold])\n",
    "# # print('w = ', w.flatten())\n",
    "# # print('b = ', b[0])\n",
    "\n",
    "# # suppV = (suppX, suppy)\n",
    "# # print(\"Support vectors: \", suppV)\n",
    "# print(\"Support vectors: \", sorted(suppX, key=lambda x: x[0]))\n",
    "# print(\"CVX score: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a089aa-5b55-422a-9449-9e7e1ca85e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd69c52-5103-4bfe-97d4-a3df07fc4928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c6daa-a677-4785-811f-dd46bcd223ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda5d43-e9c9-4f3f-9406-b238ba6bbf7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7863a37c-35e3-4dc8-bea8-b01d0e4b75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=SettingWithCopyWarning)\n",
    "# warnings.resetwarnings()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81960f5-7fbd-411c-a622-ab39e56ce844",
   "metadata": {},
   "source": [
    "# DEATH BY PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39584294-aa51-460d-acf1-428fd938ab8f",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f0eb4-1c3f-4c53-8573-d3c5ab460399",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '2019EE10143.csv'\n",
    "split_frac = 0.8\n",
    "random_state = 69420\n",
    "low_reg, high_reg = -5,5\n",
    "low_cof, high_cof = -3,3\n",
    "low_deg, high_deg = 1, 6\n",
    "\n",
    "reg_pam = np.logspace(low_reg, high_reg, num=1+high_reg-low_reg)\n",
    "ker_coeff_pam = np.logspace(low_cof, high_cof, num=1+high_cof-low_cof)\n",
    "deg_pam = np.linspace(low_deg, high_deg, 1+low_deg+high_deg)\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_name, header=None)\n",
    "pairs = [(0,1),(4,6),(8,9)]\n",
    "features = [10, 25]\n",
    "\n",
    "for (lab1, lab2) in pairs:\n",
    "    for num_ft in features:\n",
    "        print(\"#################################################################################\")\n",
    "        print(\"Labels: \",lab1, \", \", lab2)\n",
    "        print(\"Number of features: \", num_ft)\n",
    "        print(\"#################################################################################\")\n",
    "\n",
    "        #CONVERT TO USE\n",
    "        df_temp = df.loc[df[25].isin([lab1, lab2])]\n",
    "        #print(len(df_temp))\n",
    "        df_temp.iloc[df_temp[25] == lab1, 25] = -1\n",
    "        df_temp.iloc[df_temp[25] == lab2, 25] = 1\n",
    "        df_temp = df_temp.sample(frac=1., random_state=random_state)\n",
    "        \n",
    "        # SPLIT IN TRAIN AND TEST\n",
    "        train_df = df_temp[:int(split_frac*len(df_temp))]\n",
    "        test_df = df_temp[int(split_frac*len(df_temp)):]\n",
    "        \n",
    "        # SPLIT BY FEATURES\n",
    "        X_train_temp = train_df.loc[:, [i for i in range(num_ft)]]\n",
    "        y_train_temp = train_df.loc[:, [25]]\n",
    "        X_test_temp = test_df.loc[:, [i for i in range(num_ft)]]\n",
    "        y_test_temp = test_df.loc[:, [25]]\n",
    "\n",
    "\n",
    "\n",
    "        train_X = np.array(X_train_temp.values)\n",
    "        train_y = np.array(y_train_temp.values)\n",
    "        test_X = np.array(X_test_temp.values)\n",
    "        test_y = np.array(y_test_temp.values)\n",
    "\n",
    "        print (\"Number of training examples: \", train_X.shape, train_y.shape)\n",
    "        print (\"Number of test examples: \", test_X.shape, test_y.shape)\n",
    "\n",
    "\n",
    "        # SVM STUFF & GRID SEARCH CV\n",
    "        print(\"--------------------------LIBSVM-----------------------------\")\n",
    "        ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='linear'))]\n",
    "        pipeline = Pipeline(ppl) \n",
    "        parameters = {'SVM__C':reg_pam} #Linear\n",
    "        #parameters = {'SVM__C':reg_pam, 'SVM__degree':deg_pam, 'SVM__gamma':ker_coeff_pam} #Poly\n",
    "        # parameters = {'SVM__C':reg_pam, 'SVM__degree':deg_pam, 'SVM__gamma':ker_coeff_pam}\n",
    "        grid = GridSearchCV(pipeline, param_grid=parameters, cv=5, return_train_score=True)\n",
    "\n",
    "        grid.fit(train_X, np.ravel(train_y, order='C'))\n",
    "        #print(grid.score(train_X, train_y))\n",
    "        #print(grid.score(test_X, test_y))\n",
    "        bestpar = grid.best_params_\n",
    "        print(\"The Best parameters according to grid search are: \", bestpar)\n",
    "\n",
    "        mod = svm.SVC(kernel='linear', C = bestpar['SVM__C']) #Linear\n",
    "        #mod = svm.SVC(kernel='poly', C = bestpar['SVM__C'], gamma = bestpar['SVM__gamma'], degree= bestpar['SVM__degree']) #Poly\n",
    "        \n",
    "        mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "        print(\"Training score for LIBSVM with best parameters: \", 100*mod.score(train_X, train_y), \"%\")\n",
    "        print(\"Test score for LIBSVM with best parameters: \", 100*mod.score(test_X, test_y), \"%\")\n",
    "        #print(\"Support vectors as returned by LIBSVM: \", sorted(mod.support_vectors_, key=lambda x: x[0]))\n",
    "        ab,a_ind,b_ind=np.intersect1d(train_X, mod.support_vectors_, return_indices=True)\n",
    "        boi = []\n",
    "        a_ind = sorted(a_ind)\n",
    "        for i in range(0, len(a_ind), num_ft):\n",
    "            boi.append(a_ind[i]//num_ft)\n",
    "        #print(\"Indices of support vectors as returned by LIBSVM: \", [temp_train_X.index(x) for x in mod.support_vectors_])\n",
    "        print(\"Indices of support vectors as returned by LIBSVM: \", boi)\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        # CVXOPT STUFF\n",
    "        print(\"--------------------------CVXOPT-----------------------------\")\n",
    "        legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which='linear', C=bestpar['SVM__C']) #Linear\n",
    "        #legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which='poly', C=bestpar['SVM__C'],  #Poly pt1\n",
    "        #                                             gamma = bestpar['SVM__gamma'], degree= bestpar['SVM__degree']) #Poly pt2\n",
    "\n",
    "        print(\"Test score for CVXOPT with best parameters: \", acc, \"%\")\n",
    "        ab,a_ind,b_ind=np.intersect1d(train_X, suppX, return_indices=True)\n",
    "        boi = []\n",
    "        a_ind = sorted(a_ind)\n",
    "        for i in range(0, len(a_ind), num_ft):\n",
    "            boi.append(a_ind[i]//num_ft)\n",
    "        print(\"Indices of support vectors as returned by CVXOPT: \", boi)\n",
    "        #print(\"Support vectors as returned by CVXOPT: \", sorted(suppX, key=lambda x: x[0]))\n",
    "\n",
    "        \n",
    "        \n",
    "        # PLOTTINGS\n",
    "        plot_it_all_linear(train_X, train_y, test_X, test_y, reg_pam) #Linear. Nothing for anything else. Can use other one for RBF\n",
    "        plot_search_results_linear(grid, reg_pam) #Linear\n",
    "        #plot_search_results(grid) #For non-linear or any with multiple parameters\n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce73fab-a4da-4c71-9cb9-de0eed3cb7b8",
   "metadata": {},
   "source": [
    "## Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a447d-6090-4039-8ee8-4172aeec6bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################################################\n",
      "Labels: 8 , 9\n",
      "Number of features: 10\n",
      "#################################################################################\n",
      "Number of training examples: (454, 10) (454, 1)\n",
      "Number of test examples: (114, 10) (114, 1)\n",
      "--------------------------LIBSVM-----------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = '2019EE10143.csv'\n",
    "split_frac = 0.8\n",
    "random_state = 69420\n",
    "low_reg, high_reg = -3,3\n",
    "low_cof, high_cof = -3,3\n",
    "low_deg, high_deg = 1, 6\n",
    "\n",
    "reg_pam = np.logspace(low_reg, high_reg, num=1+high_reg-low_reg)\n",
    "ker_coeff_pam = np.logspace(low_cof, high_cof, num=1+high_cof-low_cof)\n",
    "deg_pam = np.linspace(low_deg, high_deg, 1+high_deg-low_deg)\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_name, header=None)\n",
    "pairs = [(0,1),(4,6),(8,9)]\n",
    "# pairs = [(8,9)]\n",
    "features = [10, 25]\n",
    "\n",
    "for (lab1, lab2) in pairs:\n",
    "    for num_ft in features:\n",
    "        print(\"#################################################################################\")\n",
    "        print(\"Labels:\",lab1, \",\", lab2)\n",
    "        print(\"Number of features:\", num_ft)\n",
    "        print(\"#################################################################################\")\n",
    "\n",
    "        #CONVERT TO USE\n",
    "        df_temp = df.loc[df[25].isin([lab1, lab2])]\n",
    "        #print(len(df_temp))\n",
    "        df_temp.iloc[df_temp[25] == lab1, 25] = -1\n",
    "        df_temp.iloc[df_temp[25] == lab2, 25] = 1\n",
    "        df_temp = df_temp.sample(frac=1., random_state=random_state)\n",
    "        \n",
    "        # SPLIT IN TRAIN AND TEST\n",
    "        train_df = df_temp[:int(split_frac*len(df_temp))]\n",
    "        test_df = df_temp[int(split_frac*len(df_temp)):]\n",
    "        \n",
    "        # SPLIT BY FEATURES\n",
    "        X_train_temp = train_df.loc[:, [i for i in range(num_ft)]]\n",
    "        y_train_temp = train_df.loc[:, [25]]\n",
    "        X_test_temp = test_df.loc[:, [i for i in range(num_ft)]]\n",
    "        y_test_temp = test_df.loc[:, [25]]\n",
    "\n",
    "\n",
    "\n",
    "        train_X = np.array(X_train_temp.values)\n",
    "        train_y = np.array(y_train_temp.values)\n",
    "        test_X = np.array(X_test_temp.values)\n",
    "        test_y = np.array(y_test_temp.values)\n",
    "\n",
    "        print (\"Number of training examples:\", train_X.shape, train_y.shape)\n",
    "        print (\"Number of test examples:\", test_X.shape, test_y.shape)\n",
    "\n",
    "\n",
    "        # SVM STUFF & GRID SEARCH CV\n",
    "        print(\"--------------------------LIBSVM-----------------------------\")\n",
    "        ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='poly'))]\n",
    "        pipeline = Pipeline(ppl) \n",
    "        #parameters = {'SVM__C':reg_pam} #Linear\n",
    "        parameters = {'SVM__C':reg_pam, 'SVM__degree':deg_pam, 'SVM__gamma':ker_coeff_pam} #Poly\n",
    "        # parameters = {'SVM__C':reg_pam, 'SVM__degree':deg_pam, 'SVM__gamma':ker_coeff_pam}\n",
    "        grid = GridSearchCV(pipeline, param_grid=parameters, cv=5, return_train_score=True, verbose=0)\n",
    "\n",
    "        grid.fit(train_X, np.ravel(train_y, order='C'))\n",
    "        #print(grid.score(train_X, train_y))\n",
    "        #print(grid.score(test_X, test_y))\n",
    "        bestpar = grid.best_params_\n",
    "        print(\"The Best parameters according to grid search are:\", bestpar)\n",
    "\n",
    "        #mod = svm.SVC(kernel='linear', C = bestpar['SVM__C']) #Linear\n",
    "        mod = svm.SVC(kernel='poly', C = bestpar['SVM__C'], gamma = bestpar['SVM__gamma'], degree= bestpar['SVM__degree']) #Poly\n",
    "        \n",
    "        mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "        print(\"Training score for LIBSVM with best parameters:\", 100*mod.score(train_X, train_y), \"%\")\n",
    "        print(\"Test score for LIBSVM with best parameters:\", 100*mod.score(test_X, test_y), \"%\")\n",
    "        ab,a_ind,b_ind=np.intersect1d(train_X, mod.support_vectors_, return_indices=True)\n",
    "        a_ind = sorted(a_ind)\n",
    "        boi = []\n",
    "        for i in range(0, len(a_ind), num_ft):\n",
    "            boi.append(a_ind[i]//num_ft)\n",
    "        #print(\"Indices of support vectors as returned by LIBSVM: \", [temp_train_X.index(x) for x in mod.support_vectors_])\n",
    "        print(\"Indices of support vectors as returned by LIBSVM: \", boi)\n",
    "        #print(\"Support vectors as returned by LIBSVM:\", sorted(mod.support_vectors_, key=lambda x: x[0]))\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        # CVXOPT STUFF\n",
    "        print(\"--------------------------CVXOPT-----------------------------\")\n",
    "        #legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which='linear', C=bestpar['SVM__C']) #Linear\n",
    "        legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which='poly', C=bestpar['SVM__C'], \n",
    "                                                     gamma = bestpar['SVM__gamma'], degree= bestpar['SVM__degree']) #Poly\n",
    "\n",
    "        print(\"Test score for CVXOPT with best parameters:\", acc, \"%\")\n",
    "        ab,a_ind,b_ind=np.intersect1d(train_X, suppX, return_indices=True)\n",
    "        a_ind = sorted(a_ind)\n",
    "        boi = []\n",
    "        for i in range(0, len(a_ind), num_ft):\n",
    "            boi.append(a_ind[i]//num_ft)\n",
    "        #print(\"Indices of support vectors as returned by LIBSVM: \", [temp_train_X.index(x) for x in mod.support_vectors_])\n",
    "        print(\"Indices of support vectors as returned by CVXOPT: \", boi)\n",
    "        #print(\"Support vectors as returned by CVXOPT:\", sorted(suppX, key=lambda x: x[0]))\n",
    "\n",
    "        \n",
    "        \n",
    "        # PLOTTINGS\n",
    "        #plot_it_all_linear(train_X, train_y, test_X, test_y, reg_pam) #Linear. Nothing for anything else. Can use other one for RBF\n",
    "        #plot_search_results_linear(grid, reg_pam) #Linear\n",
    "        plot_search_results(grid) #For non-linear or any with multiple parameters\n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66300860-efa8-418e-8fe5-90269fe0ef4c",
   "metadata": {},
   "source": [
    "## RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d7a31-878d-4960-880f-2102efcee625",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '2019EE10143.csv'\n",
    "split_frac = 0.8\n",
    "random_state = 69420\n",
    "low_reg, high_reg = -3,3\n",
    "low_cof, high_cof = -3,3\n",
    "low_deg, high_deg = 1, 6\n",
    "\n",
    "reg_pam = np.logspace(low_reg, high_reg, num=1+high_reg-low_reg)\n",
    "ker_coeff_pam = np.logspace(low_cof, high_cof, num=1+high_cof-low_cof)\n",
    "deg_pam = np.linspace(low_deg, high_deg, 1+high_deg-low_deg)\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_name, header=None)\n",
    "pairs = [(0,1),(4,6),(8,9)]\n",
    "features = [10, 25]\n",
    "\n",
    "for (lab1, lab2) in pairs:\n",
    "    for num_ft in features:\n",
    "        print(\"#################################################################################\")\n",
    "        print(\"Labels:\",lab1, \",\", lab2)\n",
    "        print(\"Number of features:\", num_ft)\n",
    "        print(\"#################################################################################\")\n",
    "\n",
    "        #CONVERT TO USE\n",
    "        df_temp = df.loc[df[25].isin([lab1, lab2])]\n",
    "        #print(len(df_temp))\n",
    "        df_temp.iloc[df_temp[25] == lab1, 25] = -1\n",
    "        df_temp.iloc[df_temp[25] == lab2, 25] = 1\n",
    "        df_temp = df_temp.sample(frac=1., random_state=random_state)\n",
    "        \n",
    "        # SPLIT IN TRAIN AND TEST\n",
    "        train_df = df_temp[:int(split_frac*len(df_temp))]\n",
    "        test_df = df_temp[int(split_frac*len(df_temp)):]\n",
    "        \n",
    "        # SPLIT BY FEATURES\n",
    "        X_train_temp = train_df.loc[:, [i for i in range(num_ft)]]\n",
    "        y_train_temp = train_df.loc[:, [25]]\n",
    "        X_test_temp = test_df.loc[:, [i for i in range(num_ft)]]\n",
    "        y_test_temp = test_df.loc[:, [25]]\n",
    "\n",
    "\n",
    "\n",
    "        train_X = np.array(X_train_temp.values)\n",
    "        train_y = np.array(y_train_temp.values)\n",
    "        test_X = np.array(X_test_temp.values)\n",
    "        test_y = np.array(y_test_temp.values)\n",
    "\n",
    "        print (\"Number of training examples:\", train_X.shape, train_y.shape)\n",
    "        print (\"Number of test examples:\", test_X.shape, test_y.shape)\n",
    "\n",
    "\n",
    "        # SVM STUFF & GRID SEARCH CV\n",
    "        print(\"--------------------------LIBSVM-----------------------------\")\n",
    "        ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='rbf'))]\n",
    "        pipeline = Pipeline(ppl) \n",
    "        #parameters = {'SVM__C':reg_pam} #Linear\n",
    "        #parameters = {'SVM__C':reg_pam, 'SVM__degree':deg_pam, 'SVM__gamma':ker_coeff_pam} #Poly\n",
    "        parameters = {'SVM__C':reg_pam, 'SVM__gamma':ker_coeff_pam} #RBF\n",
    "        grid = GridSearchCV(pipeline, param_grid=parameters, cv=5, return_train_score=True, verbose=0)\n",
    "\n",
    "        grid.fit(train_X, np.ravel(train_y, order='C'))\n",
    "        #print(grid.score(train_X, train_y))\n",
    "        #print(grid.score(test_X, test_y))\n",
    "        bestpar = grid.best_params_\n",
    "        print(\"The Best parameters according to grid search are:\", bestpar)\n",
    "\n",
    "        #mod = svm.SVC(kernel='linear', C = bestpar['SVM__C']) #Linear\n",
    "        #mod = svm.SVC(kernel='poly', C = bestpar['SVM__C'], gamma = bestpar['SVM__gamma'], degree= bestpar['SVM__degree']) #Poly\n",
    "        mod = svm.SVC(kernel='rbf', C = bestpar['SVM__C'], gamma = bestpar['SVM__gamma']) #RBF\n",
    "        \n",
    "        mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "        print(\"Training score for LIBSVM with best parameters:\", 100*mod.score(train_X, train_y), \"%\")\n",
    "        print(\"Test score for LIBSVM with best parameters:\", 100*mod.score(test_X, test_y), \"%\")\n",
    "        ab,a_ind,b_ind=np.intersect1d(train_X, mod.support_vectors_, return_indices=True)\n",
    "        a_ind = sorted(a_ind)\n",
    "        boi = []\n",
    "        for i in range(0, len(a_ind), num_ft):\n",
    "            boi.append(a_ind[i]//num_ft)\n",
    "        #print(\"Indices of support vectors as returned by LIBSVM: \", [temp_train_X.index(x) for x in mod.support_vectors_])\n",
    "        print(\"Indices of support vectors as returned by LIBSVM: \", boi)\n",
    "        #print(\"Support vectors as returned by LIBSVM:\", sorted(mod.support_vectors_, key=lambda x: x[0]))\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        # CVXOPT STUFF\n",
    "        print(\"--------------------------CVXOPT-----------------------------\")\n",
    "        #legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which='linear', C=bestpar['SVM__C']) #Linear\n",
    "        #legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which='rbf', C=bestpar['SVM__C'], \n",
    "        #                                             gamma = bestpar['SVM__gamma'], degree= bestpar['SVM__degree']) #Poly\n",
    "        legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which='rbf', C=bestpar['SVM__C'], gamma = bestpar['SVM__gamma']) #RBF\n",
    "\n",
    "        print(\"Test score for CVXOPT with best parameters:\", acc, \"%\")\n",
    "        ab,a_ind,b_ind=np.intersect1d(train_X, suppX, return_indices=True)\n",
    "        a_ind = sorted(a_ind)\n",
    "        boi = []\n",
    "        for i in range(0, len(a_ind), num_ft):\n",
    "            boi.append(a_ind[i]//num_ft)\n",
    "        #print(\"Indices of support vectors as returned by LIBSVM: \", [temp_train_X.index(x) for x in mod.support_vectors_])\n",
    "        print(\"Indices of support vectors as returned by CVXOPT: \", boi)\n",
    "        #print(\"Support vectors as returned by CVXOPT:\", sorted(suppX, key=lambda x: x[0]))\n",
    "\n",
    "        \n",
    "        \n",
    "        # PLOTTINGS\n",
    "        #plot_it_all_linear(train_X, train_y, test_X, test_y, reg_pam) #Linear. Nothing for anything else. Can use other one for RBF\n",
    "        plot_it_all_rbf(train_X, train_y, test_X, test_y, reg_pam, ker_coeff_pam)\n",
    "        #plot_search_results_linear(grid, reg_pam) #Linear\n",
    "        plot_search_results(grid) #For non-linear or any with multiple parameters\n",
    "        \n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1728c10-b3d2-4c37-9835-8a6345df9370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
