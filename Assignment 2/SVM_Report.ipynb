{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae78186-7827-44d6-bc5d-45080db3b8a1",
   "metadata": {},
   "source": [
    "## Imports and data-reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6074cef-fa8f-4d97-a329-6688f440869b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# !pip install cvxopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxopt import matrix, solvers\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a2f15-9e5d-4d03-a593-228e9a32e115",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Useful Functions for any part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c5dc98-0282-4b2d-a7e9-88c494ebd714",
   "metadata": {
    "tags": []
   },
   "source": [
    "### For SVM/SVC plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd494fc1-02a2-46c7-80a4-f4c89cb13301",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_it_all_rbf(train_X, train_y, test_X, test_y, reg_pam, ker_coeff_pam):\n",
    "\n",
    "    ######################################### 3D plot ########################################\n",
    "    Z = np.ones((len(reg_pam),len(ker_coeff_pam)))\n",
    "    W = np.ones((len(reg_pam),len(ker_coeff_pam)))\n",
    "    X, Y = np.meshgrid(reg_pam, ker_coeff_pam)\n",
    "\n",
    "\n",
    "    for idxc, c in enumerate(reg_pam):\n",
    "        for idxg, g in enumerate(ker_coeff_pam):\n",
    "            ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='poly', C = c, gamma = g))]\n",
    "            pipeline = Pipeline(ppl) \n",
    "            mod = pipeline\n",
    "#             mod = svm.SVC(kernel='poly', C = c, gamma = g)\n",
    "            mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "            Z[idxc][idxg] = mod.score(train_X, train_y)\n",
    "            W[idxc][idxg] = mod.score(test_X, test_y)\n",
    "\n",
    "    # print(X, Y, Z, W, X.shape, Y.shape, Z.shape, W.shape)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_wireframe(np.log10(X), np.log10(Y), Z, rstride=1, cstride=1, label='train', color='blue' )\n",
    "    ax.plot_wireframe(np.log10(X), np.log10(Y), W, rstride=1, cstride=1, label='test', color='red')\n",
    "    #ax.plot_surface(np.log10(X), np.log10(Y), Z, rstride=1, cstride=1, label='train', cmap='viridis', edgecolor='none' )\n",
    "    #ax.plot_surface(np.log10(X), np.log10(Y), W, rstride=1, cstride=1, label='test', cmap='viridis', edgecolor='none')\n",
    "    #ax.contour(np.log10(X), np.log10(Y), Z, label='train', cmap='viridis')\n",
    "    #ax.contour(np.log10(X), np.log10(Y), W, label='test', cmap='viridis')\n",
    "    #ax.colorbar()\n",
    "    ax.set_title('Poly kernel: C and Gamma Vs Accuracy')\n",
    "    ax.set_xlabel('log(C)')\n",
    "    ax.set_ylabel('log(Gamma)')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    ######################################## My conventional plots ########################################\n",
    "    for idxc, c in enumerate(reg_pam):\n",
    "        E_score_train = []\n",
    "        E_score_test = []\n",
    "\n",
    "        for idxg, g in enumerate(ker_coeff_pam):\n",
    "            ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='poly', C = c, gamma = g))]\n",
    "            pipeline = Pipeline(ppl) \n",
    "            mod = pipeline\n",
    "            #mod = svm.SVC(kernel='poly', C = c, gamma = g)\n",
    "            mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "            E_score_train.append(mod.score(train_X, train_y))\n",
    "            E_score_test.append(mod.score(test_X, test_y))\n",
    "\n",
    "        plt.plot(ker_coeff_pam, E_score_test, \"r-\", label=\"Test\")\n",
    "        plt.plot(ker_coeff_pam, E_score_train, \"b-\", label=\"Train\")\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(r'$\\Gamma$')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        plt.title('Score for C=%s'%(c))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "def plot_it_all_linear(train_X, train_y, test_X, test_y, reg_pam):\n",
    "\n",
    "    ######################################## My conventional plots ########################################\n",
    "    E_score_train = []\n",
    "    E_score_test = []\n",
    "\n",
    "    for idxc, c in enumerate(reg_pam):    \n",
    "        ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='linear', C = c))]\n",
    "        pipeline = Pipeline(ppl) \n",
    "        mod = pipeline\n",
    "#         mod = svm.SVC(kernel='poly', C = c, gamma = g)\n",
    "        mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "        E_score_train.append(mod.score(train_X, train_y))\n",
    "        E_score_test.append(mod.score(test_X, test_y))\n",
    "\n",
    "    plt.plot(reg_pam, E_score_test, \"r-\", label=\"Test\")\n",
    "    plt.plot(reg_pam, E_score_train, \"b-\", label=\"Train\")\n",
    "    plt.title(\"Score Vs C for LIBSVM\")\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('C')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "#     plt.title('Score for C=%s'%(c))\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "######################################## PLOTS FOR GRID SEARCH ########################################\n",
    "\n",
    "def plot_search_results(grid):\n",
    "    # Reference: https://stackoverflow.com/questions/37161563/how-to-graph-grid-scores-from-gridsearchcv\n",
    "    \"\"\"\n",
    "    Params: \n",
    "        grid: A trained GridSearchCV object.\n",
    "    For plotting the results when tuning several hyperparameters, \n",
    "    I fixed all parameters to their best value except for one and \n",
    "    plotted the mean score for the other parameter for each of its values.\n",
    "    \"\"\"\n",
    "    ## Results from grid search\n",
    "    results = grid.cv_results_\n",
    "    means_test = results['mean_test_score']\n",
    "    stds_test = results['std_test_score']\n",
    "    means_train = results['mean_train_score']\n",
    "    stds_train = results['std_train_score']\n",
    "\n",
    "    ## Getting indexes of values per hyper-parameter\n",
    "    masks=[]\n",
    "    masks_names= list(grid.best_params_.keys())\n",
    "    for p_k, p_v in grid.best_params_.items():\n",
    "        masks.append(list(results['param_'+p_k].data==p_v))\n",
    "\n",
    "    params=grid.param_grid\n",
    "\n",
    "    ## Ploting results\n",
    "    fig, ax = plt.subplots(1,len(params),sharex='none', sharey=False,figsize=(len(params)*10,len(params)*2.5))\n",
    "    fig.suptitle('Score per parameter')\n",
    "    fig.text(0.085, 0.5, 'MEAN SCORE', va='center', rotation='vertical')\n",
    "    pram_preformace_in_best = {}\n",
    "    for i, p in enumerate(masks_names):\n",
    "        m = np.stack(masks[:i] + masks[i+1:])\n",
    "#         m=np.array(masks[:i])\n",
    "        pram_preformace_in_best\n",
    "        best_parms_mask = m.all(axis=0)\n",
    "        best_index = np.where(best_parms_mask)[0]\n",
    "        x = np.array(params[p])\n",
    "        y_1 = np.array(means_test[best_index])\n",
    "        e_1 = np.array(stds_test[best_index])\n",
    "        y_2 = np.array(means_train[best_index])\n",
    "        e_2 = np.array(stds_train[best_index])\n",
    "        if (x[-1]>=x[-2]*9):\n",
    "            ax[i].set_xscale('log')\n",
    "        ax[i].errorbar(x, y_1, e_1, linestyle='--', marker='o', label='test')\n",
    "        ax[i].errorbar(x, y_2, e_2, linestyle='-', marker='^',label='train' )\n",
    "        ax[i].set_xlabel(p.upper())\n",
    "        ax[i].grid(True)\n",
    "        ax[i].legend()\n",
    "\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "def plot_search_results_linear(grid, reg_pam):\n",
    "    results = grid.cv_results_\n",
    "    means_test = results['mean_test_score']\n",
    "    stds_test = results['std_test_score']\n",
    "    means_train = results['mean_train_score']\n",
    "    stds_train = results['std_train_score']\n",
    "\n",
    "#     print(\"\")\n",
    "    # print('mean_test_score: ', means_test)\n",
    "    # print('std_test_score: ', stds_test)\n",
    "    # print('mean_train_score: ', means_train)\n",
    "    # print('std_train_score: ', stds_train)\n",
    "    x = reg_pam\n",
    "    y_1 = np.array(means_test)\n",
    "    e_1 = np.array(stds_test)\n",
    "    y_2 = np.array(means_train)\n",
    "    e_2 = np.array(stds_train)\n",
    "    plt.errorbar(x, y_1, e_1, linestyle='--', marker='o', label='test')\n",
    "    plt.errorbar(x, y_2, e_2, linestyle='-', marker='^',label='train' )\n",
    "    plt.grid(True)\n",
    "    plt.xscale('log')\n",
    "    plt.title('Score Vs C for CVXOPT')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('C')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af1107f-483f-4ba8-b0d7-fda77a06b0ab",
   "metadata": {},
   "source": [
    "### For CVXOPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b1931d-8e9a-46d5-9acf-7bee17e42566",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://courses.csail.mit.edu/6.867/wiki/images/a/a7/Qp-cvxopt.pdf\n",
    "# Reference: https://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf\n",
    "\n",
    "def cvx_try(X, y, X_test, y_test, which, C=1.0, gamma=0.04, coef0= 0.0, degree=3, threshold=1e-4):\n",
    "    \n",
    "    # Linear: C\n",
    "    # Poly: C, Gamma, Degree, Coef0\n",
    "    # Sigmoid: C, Gamma, Coef0\n",
    "    # RBF: C, gamma\n",
    "    \n",
    "    def makeHP(X1, X2, y_temp, which):\n",
    "        H = np.dot(X1, X2.T)\n",
    "        if (which=='linear'):\n",
    "            P = matrix(y_temp.dot(y_temp.T)*H)\n",
    "        elif (which=='poly'):\n",
    "            H = (gamma*H+coef0)**degree\n",
    "            P = matrix(y_temp.dot(y_temp.T)*H)\n",
    "        elif (which=='sigmoid'):\n",
    "            H = np.tanh(gamma*H+coef0)\n",
    "            P = matrix(y_temp.dot(y_temp.T)*H)\n",
    "        elif (which=='rbf'):\n",
    "            H1 = np.diag(X1.dot(X1.T)).reshape(-1, 1)*np.ones((1, X2.shape[0]))\n",
    "            H2 = np.diag(X2.dot(X2.T)).reshape(1, -1)*np.ones((X1.shape[0],1))\n",
    "            H = 2*H-H1-H2\n",
    "            H = np.exp(gamma*H)\n",
    "            P = matrix(y_temp.dot(y_temp.T)*H)\n",
    "    \n",
    "        return H,P\n",
    "    \n",
    "    y_temp = y.reshape(-1, 1)*1.\n",
    "    H,P = makeHP(X, X, y_temp, which)\n",
    "    q = matrix(-np.ones((X.shape[0], 1)))\n",
    "    A = matrix(y_temp.reshape(1, -1))\n",
    "    b = matrix(np.zeros(1))\n",
    "    G = matrix(np.r_[-1*(np.eye(X.shape[0])), np.eye(X.shape[0])])\n",
    "    h = matrix(np.r_[np.zeros(X.shape[0]), np.ones(X.shape[0])*C])\n",
    "    #print(repr(q),repr(A),repr(b),repr(G),repr(h))\n",
    "    solvers.options['show_progress'] = False\n",
    "    sol = solvers.qp(P,q,G,h,A,b)\n",
    "    lambs, _ = np.array(sol['x']), np.array(sol['primal objective'])\n",
    "    \n",
    "    \n",
    "    ########## GET SUPPORT VECTORS ################\n",
    "    idx = np.where(lambs > threshold)[0] # Indices of support vectors\n",
    "    #Extract support vectors\n",
    "    sX = X[idx,:]\n",
    "    sy = y[idx]\n",
    "    lambs = lambs[idx]\n",
    "    b = np.sum(sy)\n",
    "    for j in idx:\n",
    "        b = b - np.sum(lambs*sy*(H[j, idx].reshape(-1, 1)))\n",
    "    b /= idx.shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######### PREDICT ###########\n",
    "    ynew = np.zeros((X_test.shape[0],))\n",
    "    Htemp,_ = makeHP(sX, test_X, ynew, which)\n",
    "    rightcnt = 0.\n",
    "#     print(ynew.shape, Htemp.shape)\n",
    "    for i in range(ynew.shape[0]):\n",
    "        ynew[i] = np.sum(lambs*sy*Htemp[:,i].reshape(-1,1))+b\n",
    "        if (ynew[i]*y_test[i]>0):\n",
    "            rightcnt+=1.\n",
    "    y_pred = np.sign(ynew)\n",
    "    score = rightcnt*100.0/ynew.shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return lambs, sX, sy, y_pred, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7eb38-91e1-4a7c-b926-285c2785479a",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfee0a-0780-4513-a87a-0e41e4f5e6bf",
   "metadata": {},
   "source": [
    "### CVX something and warnings filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2428bd38-316c-4ef0-a670-760148f7d010",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # cvx_try(X, y, X_test, y_test, which, C=1.0, gamma=0.04, coef0= 0.0, degree=3, threshold=1e-4):\n",
    "# legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which='poly', C=0.01, gamma=1000, degree=1)\n",
    "\n",
    "\n",
    "# # threshold = 1e-4\n",
    "# # ############ https://xavierbourretsicotte.github.io/SVM_implementation.html #########\n",
    "# # #w parameter in vectorized form\n",
    "# # w = ((train_y * idk).T @ train_X).reshape(-1,1)\n",
    "# # #Selecting the set of indices S corresponding to non zero parameters\n",
    "# # S = (idk > threshold).flatten()\n",
    "# # #Computing b\n",
    "# # b = train_y[S] - np.dot(train_X[S], w)\n",
    "# # #Display results\n",
    "# # # print('Alphas = ',idk[idk > threshold])\n",
    "# # print('w = ', w.flatten())\n",
    "# # print('b = ', b[0])\n",
    "\n",
    "# # suppV = (suppX, suppy)\n",
    "# # print(\"Support vectors: \", suppV)\n",
    "# print(\"Support vectors: \", sorted(suppX, key=lambda x: x[0]))\n",
    "# print(\"CVX score: \", acc)\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=SettingWithCopyWarning)\n",
    "# warnings.resetwarnings()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81960f5-7fbd-411c-a622-ab39e56ce844",
   "metadata": {},
   "source": [
    "# Part 1A (Binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a4bfc-9d6b-458e-8fb2-be97ba75cfb1",
   "metadata": {},
   "source": [
    "## Death by pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f36be-2e1e-499e-b79a-9c574b71338a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = '2019EE10143.csv'\n",
    "random_state = 69420\n",
    "# split_frac = 0.8\n",
    "# low_reg, high_reg = -3,2\n",
    "# low_cof, high_cof = -3,1\n",
    "# low_deg, high_deg = 1,4\n",
    "split_frac = 0.1\n",
    "low_reg, high_reg = -1,1\n",
    "low_cof, high_cof = -1,1\n",
    "low_deg, high_deg = 1,2\n",
    "\n",
    "reg_pam = np.logspace(low_reg, high_reg, num=1+high_reg-low_reg)\n",
    "ker_coeff_pam = np.logspace(low_cof, high_cof, num=1+high_cof-low_cof)\n",
    "deg_pam = np.linspace(low_deg, high_deg, 1+high_deg-low_deg)\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_name, header=None)\n",
    "pairs = [(0,1), (4,6), (8,9)]\n",
    "# pairs = [(8,9)]\n",
    "features = [10,25]\n",
    "typ = ['linear','poly','rbf','sigmoid']\n",
    "for (lab1, lab2) in pairs:\n",
    "    for num_ft in features:\n",
    "        for which in typ:\n",
    "            print(\"#################################################################################\")\n",
    "            print(\"Labels:\",lab1, \",\", lab2)\n",
    "            print(\"Number of features:\", num_ft)\n",
    "            print(\"#################################################################################\")\n",
    "\n",
    "            #CONVERT TO USE\n",
    "            df_temp = df.loc[df[25].isin([lab1, lab2])]\n",
    "            #print(len(df_temp))\n",
    "            df_temp.iloc[df_temp[25] == lab1, 25] = -1\n",
    "            df_temp.iloc[df_temp[25] == lab2, 25] = 1\n",
    "            df_temp = df_temp.sample(frac=1., random_state=random_state)\n",
    "\n",
    "            # SPLIT IN TRAIN AND TEST\n",
    "            train_df = df_temp[:int(split_frac*len(df_temp))]\n",
    "            test_df = df_temp[int(split_frac*len(df_temp)):]\n",
    "\n",
    "            # SPLIT BY FEATURES\n",
    "            X_train_temp = train_df.loc[:, [i for i in range(num_ft)]]\n",
    "            y_train_temp = train_df.loc[:, [25]]\n",
    "            X_test_temp = test_df.loc[:, [i for i in range(num_ft)]]\n",
    "            y_test_temp = test_df.loc[:, [25]]\n",
    "\n",
    "\n",
    "\n",
    "            train_X = np.array(X_train_temp.values)\n",
    "            train_y = np.array(y_train_temp.values)\n",
    "            test_X = np.array(X_test_temp.values)\n",
    "            test_y = np.array(y_test_temp.values)\n",
    "\n",
    "            print (\"Number of training examples:\", train_X.shape, train_y.shape)\n",
    "            print (\"Number of test examples:\", test_X.shape, test_y.shape)\n",
    "\n",
    "\n",
    "            # SVM STUFF & GRID SEARCH CV\n",
    "            print(\"--------------------------LIBSVM for {ty}-----------------------------\".format(ty=which.upper()))\n",
    "            \n",
    "            if (which=='linear'):\n",
    "                ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='linear'))]\n",
    "                parameters = {'SVM__C':reg_pam} #Linear\n",
    "            elif (which=='rbf' or which=='sigmoid'):\n",
    "                ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel=which))]\n",
    "                parameters = {'SVM__C':reg_pam, 'SVM__gamma':ker_coeff_pam} #RBF/SIGMOID\n",
    "            elif (which=='poly'):\n",
    "                ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='poly'))]\n",
    "                parameters = {'SVM__C':reg_pam, 'SVM__degree':deg_pam, 'SVM__gamma':ker_coeff_pam} #Poly\n",
    "            \n",
    "            pipeline = Pipeline(ppl) \n",
    "            #parameters = {'SVM__C':reg_pam} #Linear\n",
    "            #parameters = {'SVM__C':reg_pam, 'SVM__degree':deg_pam, 'SVM__gamma':ker_coeff_pam} #Poly\n",
    "            #parameters = {'SVM__C':reg_pam, 'SVM__degree':deg_pam, 'SVM__gamma':ker_coeff_pam}\n",
    "            grid = GridSearchCV(pipeline, param_grid=parameters, cv=5, return_train_score=True, verbose=0)\n",
    "\n",
    "            grid.fit(train_X, np.ravel(train_y, order='C'))\n",
    "            #print(grid.score(train_X, train_y))\n",
    "            #print(grid.score(test_X, test_y))\n",
    "            bestpar = grid.best_params_\n",
    "            print(\"The Best parameters according to grid search are:\", bestpar)\n",
    "            \n",
    "            \n",
    "            if (which=='linear'):\n",
    "                mod = svm.SVC(kernel='linear', C = bestpar['SVM__C']) #Linear\n",
    "            elif (which=='rbf' or which=='sigmoid'):\n",
    "                mod = svm.SVC(kernel=which, C = bestpar['SVM__C'], gamma = bestpar['SVM__gamma']) #RBF/SIGMOID\n",
    "            elif (which=='poly'):\n",
    "                mod = svm.SVC(kernel='poly', C = bestpar['SVM__C'], gamma = bestpar['SVM__gamma'], degree= bestpar['SVM__degree']) #Poly\n",
    "\n",
    "\n",
    "            mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "            print(\"Training score for LIBSVM with best parameters:\", 100*mod.score(train_X, train_y), \"%\")\n",
    "            print(\"Test score for LIBSVM with best parameters:\", 100*mod.score(test_X, test_y), \"%\")\n",
    "            ab,a_ind,b_ind=np.intersect1d(train_X, mod.support_vectors_, return_indices=True)\n",
    "            a_ind = sorted(a_ind)\n",
    "            boi = []\n",
    "            for i in range(0, len(a_ind), num_ft):\n",
    "                boi.append(a_ind[i]//num_ft)\n",
    "            \n",
    "            print(\"Indices of support vectors as returned by LIBSVM: \", boi)\n",
    "            \n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "\n",
    "            # CVXOPT STUFF\n",
    "            print(\"--------------------------CVXOPT for {ty}-----------------------------\".format(ty=which.upper()))\n",
    "            \n",
    "            flag=True\n",
    "            if (which=='linear'):\n",
    "                legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which='linear', C=bestpar['SVM__C']) #Linear\n",
    "            elif (which=='rbf' or which=='sigmoid'):\n",
    "                try:\n",
    "                    legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which=which, C=bestpar['SVM__C'], gamma = bestpar['SVM__gamma']) #RBF/Sigmoid\n",
    "                except:\n",
    "                    flag=False\n",
    "                    print(\"An exception occurred for {ty} at params C={c}, gamma={gam}\".format(ty=which.title(), c=bestpar['SVM__C'], gam=bestpar['SVM__gamma']))\n",
    "            elif (which=='poly'):\n",
    "                legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which='poly', C=bestpar['SVM__C'], gamma = bestpar['SVM__gamma'], degree= bestpar['SVM__degree']) #Poly\n",
    "\n",
    "            if (flag==True):\n",
    "                print(\"Test score for CVXOPT with best parameters:\", acc, \"%\")\n",
    "                ab,a_ind,b_ind=np.intersect1d(train_X, suppX, return_indices=True)\n",
    "                a_ind = sorted(a_ind)\n",
    "                boi = []\n",
    "                for i in range(0, len(a_ind), num_ft):\n",
    "                    boi.append(a_ind[i]//num_ft)\n",
    "                print(\"Indices of support vectors as returned by CVXOPT: \", boi)\n",
    "            \n",
    "\n",
    "\n",
    "            # PLOTTINGS\n",
    "            if (which=='linear'):\n",
    "                plot_it_all_linear(train_X, train_y, test_X, test_y, reg_pam) #Linear. Nothing for anything else. Can use other one for RBF\n",
    "                plot_search_results_linear(grid, reg_pam) #Linear\n",
    "            else:\n",
    "                plot_search_results(grid) #For non-linear or any with multiple parameters\n",
    "\n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42fa35-81b4-4a5d-b608-c37707509d23",
   "metadata": {},
   "source": [
    "# PART 1A (Multi-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab154152-0dbc-497a-9480-35c1f3586c93",
   "metadata": {},
   "source": [
    "## Death by pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8049850-4efa-4442-af5f-091074224984",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = '2019EE10143.csv'\n",
    "random_state = 69420\n",
    "split_frac = 0.8\n",
    "low_reg, high_reg = -2,1\n",
    "low_cof, high_cof = -3,1\n",
    "low_deg, high_deg = 1,4\n",
    "# split_frac = 0.1\n",
    "# low_reg, high_reg = -1,1\n",
    "# low_cof, high_cof = -1,1\n",
    "# low_deg, high_deg = 1,2\n",
    "\n",
    "reg_pam = np.logspace(low_reg, high_reg, num=1+high_reg-low_reg)\n",
    "ker_coeff_pam = np.logspace(low_cof, high_cof, num=1+high_cof-low_cof)\n",
    "deg_pam = np.linspace(low_deg, high_deg, 1+high_deg-low_deg)\n",
    "\n",
    "df = pd.read_csv(file_name, header=None)\n",
    "\n",
    "features = [10,25]\n",
    "typ = ['linear','sigmoid','rbf','poly']\n",
    "for num_ft in features:\n",
    "    for which in typ:\n",
    "        print(\"#####################################################################\")\n",
    "        print(\"Number of features:\", num_ft)\n",
    "        print(\"#####################################################################\")\n",
    "\n",
    "        #CONVERT TO USE\n",
    "        # df_temp = df.loc[df[25].isin([lab1, lab2])]\n",
    "        # print(len(df_temp))\n",
    "        # df_temp.iloc[df_temp[25] == lab1, 25] = -1\n",
    "        # df_temp.iloc[df_temp[25] == lab2, 25] = 1\n",
    "        df_temp = df.sample(frac=1., random_state=random_state)\n",
    "\n",
    "        # SPLIT IN TRAIN AND TEST\n",
    "        train_df = df_temp[:int(split_frac*len(df_temp))]\n",
    "        test_df = df_temp[int(split_frac*len(df_temp)):]\n",
    "\n",
    "        # SPLIT BY FEATURES\n",
    "        X_train_temp = train_df.loc[:, [i for i in range(num_ft)]]\n",
    "        y_train_temp = train_df.loc[:, [25]]\n",
    "        X_test_temp = test_df.loc[:, [i for i in range(num_ft)]]\n",
    "        y_test_temp = test_df.loc[:, [25]]\n",
    "\n",
    "\n",
    "\n",
    "        train_X = np.array(X_train_temp.values)\n",
    "        train_y = np.array(y_train_temp.values)\n",
    "        test_X = np.array(X_test_temp.values)\n",
    "        test_y = np.array(y_test_temp.values)\n",
    "\n",
    "        print (\"Number of training examples:\", train_X.shape, train_y.shape)\n",
    "        print (\"Number of test examples:\", test_X.shape, test_y.shape)\n",
    "\n",
    "\n",
    "        # SVM STUFF & GRID SEARCH CV\n",
    "        print(\"--------------------------LIBSVM for {ty} for {ft} features-----------------------------\".format(ty=which.upper(), ft=num_ft))\n",
    "\n",
    "        if (which=='linear'):\n",
    "            ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='linear'))]\n",
    "            parameters = {'SVM__C':reg_pam} #Linear\n",
    "        elif (which=='rbf' or which=='sigmoid'):\n",
    "            ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel=which))]\n",
    "            parameters = {'SVM__C':reg_pam, 'SVM__gamma':ker_coeff_pam} #RBF/SIGMOID\n",
    "        elif (which=='poly'):\n",
    "            ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='poly'))]\n",
    "            parameters = {'SVM__C':reg_pam, 'SVM__degree':deg_pam, 'SVM__gamma':ker_coeff_pam} #Poly\n",
    "\n",
    "        pipeline = Pipeline(ppl) \n",
    "        #parameters = {'SVM__C':reg_pam} #Linear\n",
    "        #parameters = {'SVM__C':reg_pam, 'SVM__degree':deg_pam, 'SVM__gamma':ker_coeff_pam} #Poly\n",
    "        #parameters = {'SVM__C':reg_pam, 'SVM__degree':deg_pam, 'SVM__gamma':ker_coeff_pam}\n",
    "        grid = GridSearchCV(pipeline, param_grid=parameters, cv=5, return_train_score=True, verbose=0)\n",
    "\n",
    "        grid.fit(train_X, np.ravel(train_y, order='C'))\n",
    "        bestpar = grid.best_params_\n",
    "        print(\"The Best parameters according to grid search are:\", bestpar)\n",
    "        print(\"Best training score from grid search pipeline:\",100*grid.score(train_X, train_y), \"%\")\n",
    "        print(\"Best test score from grid search pipeline:\",100*grid.score(test_X, test_y), \"%\")\n",
    "        \n",
    "\n",
    "        if (which=='linear'):\n",
    "            mod = svm.SVC(kernel='linear', C = bestpar['SVM__C']) #Linear\n",
    "        elif (which=='rbf' or which=='sigmoid'):\n",
    "            mod = svm.SVC(kernel=which, C = bestpar['SVM__C'], gamma = bestpar['SVM__gamma']) #RBF/SIGMOID\n",
    "        elif (which=='poly'):\n",
    "            mod = svm.SVC(kernel='poly', C = bestpar['SVM__C'], gamma = bestpar['SVM__gamma'], degree= bestpar['SVM__degree']) #Poly\n",
    "\n",
    "\n",
    "        mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "        print(\"Training score for LIBSVM with best parameters:\", 100*mod.score(train_X, train_y), \"%\")\n",
    "        print(\"Test score for LIBSVM with best parameters:\", 100*mod.score(test_X, test_y), \"%\")\n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "        # PLOTTINGS\n",
    "        if (which=='linear'):\n",
    "            plot_it_all_linear(train_X, train_y, test_X, test_y, reg_pam) #Linear. Nothing for anything else. Can use other one for RBF\n",
    "            plot_search_results_linear(grid, reg_pam) #Linear\n",
    "        else:\n",
    "            plot_search_results(grid) #For non-linear or any with multiple parameters\n",
    "\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9aca0-1bd2-4c6d-a29b-f6372fcdb18f",
   "metadata": {},
   "source": [
    "# PART 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1664421-62e8-4afa-9d66-ef1e66d6a1b2",
   "metadata": {},
   "source": [
    "## Simplified SMO (Edit it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c5116-eb19-4f5d-9164-96ce1bd9fb5c",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95fadb90-ba2f-4d12-8ddf-1046c5e2fdbd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simplifiedSMO(C, tol, max_passes, X, y):\n",
    "    # y = y.T\n",
    "    m,n =X.shape[0], X.shape[1]\n",
    "    alphas = np.zeros((m,1))\n",
    "    b = 0\n",
    "    passes = 0\n",
    "    while (passes < max_passes):\n",
    "        num_changed_alphas = 0\n",
    "        for i in range(m):\n",
    "            # Calculate Ei = f(xi) - yi using 2\n",
    "            fxi = ((alphas*y).T@(X@X[i,:].T))*1. + b #single value\n",
    "            # print(fxi)\n",
    "            Ei = fxi-y[i] #Single value\n",
    "            # print(y[i].shape, Ei.shape)\n",
    "            flag1 = (y[i]*Ei<-tol) and (alphas[i]<C)\n",
    "            flag2 = (y[i]*Ei>tol) and (alphas[i]>0)\n",
    "            if (flag1) or (flag2):\n",
    "                # select j != i randomly\n",
    "                j=int(random.uniform(0,m))\n",
    "                while (j==i):\n",
    "                    j=int(random.uniform(0,m))\n",
    "                # Calculate Ej = f(xj) - yj\n",
    "                fxj = ((alphas*y).T@(X@X[j,:].T))*1. + b\n",
    "                Ej = fxj - y[j]\n",
    "                # save old alphas\n",
    "                alpha_i_old, alpha_j_old = alphas[i].copy(), alphas[j].copy() #To avoid previous values from being changed. Not sure if it happens\n",
    "                # compute L and H by 10 or 11\n",
    "                if (y[i]!=y[j]):\n",
    "                    L = max(0, alphas[j] - alphas[i])\n",
    "                    H = min(C, C + alphas[j] - alphas[i])\n",
    "                else:\n",
    "                    L = max(0, alphas[j] + alphas[i] - C)\n",
    "                    H = min(C, alphas[j] + alphas[i])\n",
    "                # if L = H the continue to next i\n",
    "                if L==H:\n",
    "                    continue\n",
    "                # compute eta by 14\n",
    "                eta = 2.*(X[i,:]@X[j,:].T) - (X[i,:]@X[i,:].T) - (X[j,:]@X[j,:].T)\n",
    "                # if eta >= 0: continue to next i\n",
    "                if eta>=0:\n",
    "                    continue\n",
    "                # compute and clip new value for alpha_{j} using 12 and 15\n",
    "                alphas[j] -= y[j]*(Ei-Ej)/eta\n",
    "                if (alphas[j]>H):\n",
    "                    alphas[j]=H\n",
    "                elif (L>alphas[j]):\n",
    "                    alphas[j]=L\n",
    "                # if |alphasj - alphasold| < 1e-5 then continue to next i\n",
    "                if (abs(alphas[j]-alpha_j_old) < 0.00001):\n",
    "                    continue\n",
    "                # determine value for alpha_{i} using 16\n",
    "                alphas[i] += y[j]*y[i]*(alpha_j_old-alphas[j])\n",
    "                # compute b1 and b2 using (17) and (18) respectively.\n",
    "                b1 = b - Ei- y[i]*(alphas[i]-alpha_i_old)*(X[i,:]@X[i,:].T) - y[j]*(alphas[j]-alpha_j_old)*(X[i,:]@X[j,:].T)\n",
    "                b2 = b - Ej- y[i]*(alphas[i]-alpha_i_old)*(X[i,:]@X[j,:].T) - y[j]*(alphas[j]-alpha_j_old)*(X[j,:]@X[j,:].T)\n",
    "                # compute b by 19\n",
    "                if (0<alphas[i]) and (alphas[i]<C):\n",
    "                    b = b1\n",
    "                elif (0 < alphas[j]) and (alphas[j]<C):\n",
    "                    b = b2\n",
    "                else:\n",
    "                    b = (b1 + b2)/2.                     \n",
    "                num_changed_alphas+=1\n",
    "        if (num_changed_alphas==0):\n",
    "            passes+=1\n",
    "        else: \n",
    "            passes=0\n",
    "            \n",
    "    return alphas, b\n",
    "\n",
    "def iNeedWeight(alphas, X, y):\n",
    "    \"\"\"\n",
    "    w = sum(i=1 to m) yi*alphai*xi\n",
    "    \"\"\"\n",
    "    wt = np.zeros((X.shape[1],))\n",
    "    # print(alphas[0].shape, y[0].shape, X[0,:].T.shape)\n",
    "    for i in range(X.shape[0]):\n",
    "        wt += (alphas[i]*y[i])*(X[i,:].T)\n",
    "    return wt\n",
    "\n",
    "def predict(newX, wt, b):\n",
    "    cl = (wt.T@newX+b>0)\n",
    "    if (cl):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8533c374-cbb8-462a-9f54-7a95128d68c8",
   "metadata": {},
   "source": [
    "### Death by Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4ba6e-44ce-41b9-bf2c-420f836374e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = '2019EE10143.csv'\n",
    "df = pd.read_csv(file_name, header=None)\n",
    "\n",
    "random_state = 69420\n",
    "split_frac = 0.8\n",
    "c=0.1\n",
    "pairs = [(0,1), (4,6), (8,9)]\n",
    "features = [10,25]\n",
    "for (lab1, lab2) in pairs:\n",
    "    for num_ft in features:\n",
    "        print(\"#################################################################################\")\n",
    "        print(\"Labels:\",lab1, \",\", lab2)\n",
    "        print(\"Number of features:\", num_ft)\n",
    "        print(\"#################################################################################\")\n",
    "\n",
    "        #CONVERT TO USE\n",
    "        df_temp = df.loc[df[25].isin([lab1, lab2])]\n",
    "        #print(len(df_temp))\n",
    "        df_temp.iloc[df_temp[25] == lab1, 25] = -1\n",
    "        df_temp.iloc[df_temp[25] == lab2, 25] = 1\n",
    "        df_temp = df_temp.sample(frac=1., random_state=random_state)\n",
    "        # SPLIT IN TRAIN AND TEST\n",
    "        train_df = df_temp[:int(split_frac*len(df_temp))]\n",
    "        test_df = df_temp[int(split_frac*len(df_temp)):]\n",
    "        # SPLIT BY FEATURES\n",
    "        X_train_temp = train_df.loc[:, [i for i in range(num_ft)]]\n",
    "        y_train_temp = train_df.loc[:, [25]]\n",
    "        X_test_temp = test_df.loc[:, [i for i in range(num_ft)]]\n",
    "        y_test_temp = test_df.loc[:, [25]]\n",
    "        \n",
    "        train_X = np.array(X_train_temp.values)\n",
    "        train_y = np.array(y_train_temp.values)\n",
    "        test_X = np.array(X_test_temp.values)\n",
    "        test_y = np.array(y_test_temp.values)\n",
    "        \n",
    "        print (\"Number of training examples:\", train_X.shape, train_y.shape)\n",
    "        print (\"Number of test examples:\", test_X.shape, test_y.shape)\n",
    "        \n",
    "        start = time.process_time()\n",
    "        alphas, b = simplifiedSMO(c, 0.001, 25, train_X, train_y)\n",
    "        end = time.process_time()\n",
    "        wt = iNeedWeight(alphas,train_X, train_y)\n",
    "        y_pred = np.zeros((train_X.shape[0]))\n",
    "        rightcnt = 0.\n",
    "        for idx, x in enumerate(train_X):\n",
    "            y_pred[idx] = predict(x, wt, b)\n",
    "            if (y_pred[idx]==train_y[idx]):\n",
    "                rightcnt+=1\n",
    "        acc = rightcnt/train_X.shape[0]\n",
    "        print(\"Training accuracy by simplified SMO is:\", 100*acc, \"%\")\n",
    "\n",
    "        y_pred = np.zeros((test_X.shape[0]))\n",
    "        rightcnt = 0.\n",
    "        for idx, x in enumerate(test_X):\n",
    "            y_pred[idx] = predict(x, wt, b)\n",
    "            if (y_pred[idx]==test_y[idx]):\n",
    "                rightcnt+=1\n",
    "        acc = rightcnt/test_X.shape[0]\n",
    "        print(\"Test accuracy by simplified SMO is:\", 100*acc, \"%\")\n",
    "        print(\"Time taken by simplified SMO for labels ({l1},{l2}) for {nf} features is {tt} seconds\".format(l1=lab1, l2=lab2, nf=num_ft, tt=end-start))\n",
    "        print()\n",
    "        \n",
    "        ############################# SVM ##############################\n",
    "        mod = svm.SVC(kernel='linear', C = c)\n",
    "        start = time.process_time()\n",
    "        mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "        end = time.process_time()\n",
    "        print(\"Training accuracy for LIBSVM is:\", 100*mod.score(train_X, train_y), \"%\")\n",
    "        print(\"Test accuracy for LIBSVM is:\", 100*mod.score(test_X, test_y), \"%\")\n",
    "        print(\"Time taken by LIBSVM for labels ({l1},{l2}) for {nf} features is {tt} seconds\".format(l1=lab1, l2=lab2, nf=num_ft, tt=end-start))\n",
    "        print()\n",
    "            \n",
    "        ############################# CVX ##############################\n",
    "        start = time.process_time()\n",
    "        legr, suppX, suppy, test_pred, acc = cvx_try(train_X, train_y, test_X, test_y, which='linear', C=c) #Linear\n",
    "        end = time.process_time()\n",
    "        print(\"Test accuracy for CVXOPT is:\", acc, \"%\")\n",
    "        print(\"Time taken by CVXOPT for labels ({l1},{l2}) for {nf} features is {tt} seconds\".format(l1=lab1, l2=lab2, nf=num_ft, tt=end-start))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af855347-bfb6-43a8-837e-2c0c8176244b",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05706226-3bc6-473c-8c98-7d099cfa45b9",
   "metadata": {},
   "source": [
    "### Saving in desirable format (Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fdb9e0b-cf9b-48f6-8bd1-08748ef1c85b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save2csv(filename, Y_pred):\n",
    "    with open(filename, 'w+') as f:\n",
    "        f.write('Id,Class\\n')\n",
    "        for i in range(Y_pred.shape[0]):\n",
    "            if i+1<1000:\n",
    "                f.write('{},{:d}\\n'.format(str(i+1), int(Y_pred[i])))\n",
    "            else:\n",
    "                f.write('\\\"{:01d},{:03d}\\\",{:d}\\n'.format((i+1)//1000, (i+1)%1000, int(Y_pred[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4cb3d5-d0a8-49a5-b0a5-453950936673",
   "metadata": {},
   "source": [
    "## Basic Hyperparameter sweep over everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff256a7c-8e9a-4e2a-9dc6-35403c544178",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = 'train_set.csv'\n",
    "split_frac = 0.9\n",
    "num_ft=25\n",
    "random_state = 69420\n",
    "# low_reg, high_reg = -2,3\n",
    "# low_cof, high_cof = -3,1\n",
    "# low_deg, high_deg = 1,3\n",
    "# reg_pam = np.logspace(low_reg, high_reg, num=1+high_reg-low_reg)\n",
    "# ker_coeff_pam = np.logspace(low_cof, high_cof, num=1+high_cof-low_cof)\n",
    "# deg_pam = np.linspace(low_deg, high_deg, 1+high_deg-low_deg)\n",
    "\n",
    "low_reg, high_reg = 0.3,0.8\n",
    "low_cof, high_cof = 0.2,0.8\n",
    "low_deg, high_deg = 1,2\n",
    "reg_pam = np.logspace(low_reg, high_reg, num=7)\n",
    "ker_coeff_pam = np.logspace(low_cof, high_cof, num=7)\n",
    "deg_pam = np.linspace(low_deg, high_deg, 1+high_deg-low_deg)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_name, header=None)\n",
    "\n",
    "print(\"#################################################################################\")\n",
    "# print(\"Labels:\",lab1, \",\", lab2)\n",
    "print(\"Number of features:\", num_ft)\n",
    "print(\"#################################################################################\")\n",
    "\n",
    "#CONVERT TO USE\n",
    "df_temp = df\n",
    "#print(len(df_temp))\n",
    "# df_temp.iloc[df_temp[25] == lab1, 25] = -1\n",
    "# df_temp.iloc[df_temp[25] == lab2, 25] = 1\n",
    "df_temp = df_temp.sample(frac=1., random_state=random_state)\n",
    "\n",
    "# SPLIT IN TRAIN AND TEST\n",
    "train_df = df_temp[:int(split_frac*len(df_temp))]\n",
    "test_df = df_temp[int(split_frac*len(df_temp)):]\n",
    "\n",
    "# SPLIT BY FEATURES\n",
    "X_train_temp = train_df.loc[:, [i for i in range(num_ft)]]\n",
    "y_train_temp = train_df.loc[:, [25]]\n",
    "X_test_temp = test_df.loc[:, [i for i in range(num_ft)]]\n",
    "y_test_temp = test_df.loc[:, [25]]\n",
    "\n",
    "\n",
    "\n",
    "train_X = np.array(X_train_temp.values)\n",
    "train_y = np.array(y_train_temp.values)\n",
    "test_X = np.array(X_test_temp.values)\n",
    "test_y = np.array(y_test_temp.values)\n",
    "\n",
    "print (\"Number of training examples:\", train_X.shape, train_y.shape)\n",
    "print (\"Number of test examples:\", test_X.shape, test_y.shape)\n",
    "\n",
    "typ = ['rbf']\n",
    "# typ = ['rbf']\n",
    "for which in typ:\n",
    "    # SVM STUFF & GRID SEARCH CV\n",
    "    print(\"--------------------------LIBSVM for {ty}-----------------------------\".format(ty=which.upper()))\n",
    "    if (which=='linear'):\n",
    "        ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='linear'))]\n",
    "        parameters = {'SVM__C':reg_pam} #Linear\n",
    "    elif (which=='rbf' or which=='sigmoid'):\n",
    "        ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel=which))]\n",
    "        parameters = {'SVM__C':reg_pam, 'SVM__gamma':ker_coeff_pam} #RBF/SIGMOID\n",
    "    elif (which=='poly'):\n",
    "        ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='poly'))]\n",
    "        parameters = {'SVM__C':reg_pam, 'SVM__degree':deg_pam, 'SVM__gamma':ker_coeff_pam} #Poly\n",
    "\n",
    "    pipeline = Pipeline(ppl) \n",
    "    grid = GridSearchCV(pipeline, param_grid=parameters, cv=5, return_train_score=True, verbose=3)\n",
    "\n",
    "    grid.fit(train_X, np.ravel(train_y, order='C'))\n",
    "    print(\"Grid scaled training score for libsvm is:\", 100*grid.score(train_X, train_y), \"%\")\n",
    "    print(\"Grid scaled test score for libsvm is:\", 100*grid.score(test_X, test_y), \"%\")\n",
    "    bestpar = grid.best_params_\n",
    "    print(\"The Best parameters according to grid search are:\", bestpar)\n",
    "    \n",
    "    if (which=='linear'):\n",
    "        mod = svm.SVC(kernel='linear', C = bestpar['SVM__C']) #Linear\n",
    "    if (which=='poly'):\n",
    "        mod = svm.SVC(kernel='poly', C = bestpar['SVM__C'], gamma = bestpar['SVM__gamma'], degree= bestpar['SVM__degree']) #Poly\n",
    "    if (which=='rbf' or which=='sigmoid'):\n",
    "        mod = svm.SVC(kernel=which, C = bestpar['SVM__C'], gamma = bestpar['SVM__gamma']) #RBF/Sigmoid\n",
    "    \n",
    "    mod.fit(train_X, np.ravel(train_y, order='C'))\n",
    "    print(\"Training score for LIBSVM with best parameters:\", 100*mod.score(train_X, train_y), \"%\")\n",
    "    print(\"Test score for LIBSVM with best parameters:\", 100*mod.score(test_X, test_y), \"%\")\n",
    "    \n",
    "\n",
    "\n",
    "    # PLOTTINGS\n",
    "    if (which=='linear'):\n",
    "        plot_it_all_linear(train_X, train_y, test_X, test_y, reg_pam) #Linear. Nothing for anything else. Can use other one for RBF\n",
    "        plot_search_results_linear(grid, reg_pam) #Linear\n",
    "    # plot_it_all_rbf(train_X, train_y, test_X, test_y, reg_pam, ker_coeff_pam)\n",
    "    else:\n",
    "        plot_search_results(grid) #For non-linear or any with multiple parameters\n",
    "\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823885d-1217-4818-a476-b02283cabec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid.cv_results_\n",
    "# grid.cv_results_['params']\n",
    "# grid.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84612ec9-1eda-4229-a94f-7c2428e42f64",
   "metadata": {},
   "source": [
    "### Running for final output of best model (Manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f010ddc-791e-4ab5-8941-fb1f8657108b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1. 1. 1. ... 0. 2. 7.]\n",
      "22_10_19_26_54\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "k = str(now).split(':')\n",
    "p = k[0][-2:] + '_' + k[1] + '_' + k[2].split('.')[0]\n",
    "p=k[0].split('-')[2][:2]+'_'+k[0].split('-')[1]+'_'+p\n",
    "\n",
    "num_ft=25\n",
    "\n",
    "df = pd.read_csv(\"train_set.csv\", header=None)\n",
    "df_temp = df.sample(frac=1., random_state=random_state)\n",
    "X_train_temp = df_temp.loc[:, [i for i in range(num_ft)]]\n",
    "y_train_temp = df_temp.loc[:, [25]]\n",
    "train_X = np.array(X_train_temp.values)\n",
    "train_y = np.array(y_train_temp.values)\n",
    "\n",
    "df = pd.read_csv(\"test_set.csv\", header=None)\n",
    "df_temp = df\n",
    "test_X = np.array(df_temp.values)\n",
    "\n",
    "ppl = [('scaler', StandardScaler()), ('SVM', svm.SVC(kernel='rbf', C=100, gamma=0.1))] #1,0.1\n",
    "pipeline = Pipeline(ppl) \n",
    "pipeline.fit(train_X, np.ravel(train_y, order='C'))\n",
    "print(pipeline.score(train_X, train_y))\n",
    "pred_y = pipeline.predict(test_X)\n",
    "id = np.array([i+1 for i in range(len(pred_y))])\n",
    "print(pred_y)\n",
    "print(p)\n",
    "save2csv(\"test_\"+p+\".csv\", pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8e525-7d8a-455a-9dbe-ebc40d37dc50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
