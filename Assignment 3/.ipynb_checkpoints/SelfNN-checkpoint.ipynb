{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07412e84-02c6-4388-bbb5-d0cd5ca7f19f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5937564c-d453-48e6-85dd-e9b4f57b2a89",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb9b73-369c-43da-adb5-84182c8ff47e",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c21507-7afe-48a3-a149-b8f1b407417d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#<Something>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c54d48-ec6f-431b-ba64-4108a9cce139",
   "metadata": {},
   "source": [
    "# Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977582a9-2733-4bc8-8b31-f3e846caf558",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e8e037b-1b40-47a2-842e-c8f98ca04aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds_idx(N, nFolds, seed=42):\n",
    "    \"\"\"\n",
    "    Randomly permute [0,N] and extract indices for each fold\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    rnd_idx = np.random.permutation(N)\n",
    "    N_fold = N//nFolds\n",
    "    indices = []\n",
    "    for i in range(nFolds):\n",
    "        start = i*N_fold\n",
    "        end = min([(i+1)*N_fold, N])\n",
    "        # if (N<end):\n",
    "        #     end = N\n",
    "        indices.append(rnd_idx[start:end])\n",
    "    return indices\n",
    "\n",
    "\n",
    "def showImage(img, label):\n",
    "    ficAr = np.array(img, dtype='float')\n",
    "    roughSd = int(math.sqrt(img.size))\n",
    "    pic = picAr.reshape((roughSd, roughSd)).T\n",
    "    # plt.imshow(pix, cmap='gray')\n",
    "    plt.imshow(pix)\n",
    "    lb = str(label)\n",
    "    plt.title('label for this image is',lb)\n",
    "    # print(label)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def act_fn(typ, Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    typ -- sigmoid/RELU\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z)/RELU(z), same shape as Z\n",
    "    cache -- returns Z as well, for backprop\n",
    "    \"\"\"\n",
    "    if (typ.lower()=='sigmoid'):\n",
    "        a = 1/(1+np.exp(-Z))\n",
    "        return a, Z\n",
    "    elif (typ.lower()=='relu'):\n",
    "        a = np.maximum(0,Z)\n",
    "        assert(a.shape == Z.shape)\n",
    "        return a, Z\n",
    "\n",
    "def back_fn(typ, da, cache):\n",
    "    \n",
    "    if (typ.lower()=='relu'):\n",
    "        Z = cache\n",
    "        dZ = np.array(da, copy=True) # just converting dz to a correct object.\n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dZ[Z <= 0] = 0\n",
    "        assert (dZ.shape == Z.shape)\n",
    "        return dZ\n",
    "    \n",
    "    elif (typ.lower()=='sigmoid'):\n",
    "        Z = cache\n",
    "        s = 1/(1+np.exp(-Z))\n",
    "        dZ = da*s*(1-s)\n",
    "        assert (dZ.shape == Z.shape)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07395766-6245-4eeb-af13-9822f7806d76",
   "metadata": {},
   "source": [
    "## Class (Cuz I'm fancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822307d-325b-4a36-80a2-8af7a5adc1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, input_dim=None, output_dim=None, hidden_dim=None, num_hidden_layers=None, typs=None, seed=42):\n",
    "        # Can add error, no need\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_hid_layers = num_hidden_layers\n",
    "        assert(len(self.hidden_dim)==self.num_hid_layers)\n",
    "        self.typs = typs\n",
    "        self.layers = self.hidden_dim.copy()\n",
    "        self.layers.insert(0, self.input_dim) #Not sure if replacement is inplace\n",
    "        self.layers.append(self.output_dim)\n",
    "        assert(len(self.typs) == self.num_hid_layers+1)\n",
    "        self.params, self.outs, self.derivs = init_params(self, seed)\n",
    "        \n",
    "    def init_params(self, seed):\n",
    "        np.random.seed(seed)\n",
    "        parameters = {}\n",
    "        output = {}\n",
    "        deriv = {}\n",
    "        # \n",
    "        L = len(self.layers)\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.randn(self.layers[l],self.layers[l-1])/100 #W(^l)_{i,j} = Weight from lth to l+1th layer. i=destn, j=src idx\n",
    "            parameters['b' + str(l)] = np.zeros((self.layers[l],1))\n",
    "            output[str(l)] = np.zeros((self.layers[l],1)) # Change to none somehow\n",
    "            deriv[str(l)] = np.zeros((self.layers[l],1)) # Change to none somehow\n",
    "            \n",
    "            assert(parameters['W' + str(l)].shape == (self.layers[l], self.layers[l-1]))\n",
    "            assert(parameters['b' + str(l)].shape == (self.layers[l], 1))\n",
    "            assert(output[str(l)].shape == (self.layers[l], 1))\n",
    "            assert(deriv[str(l)].shape == (self.layers[l], 1))\n",
    "\n",
    "        return parameters, output, deriv\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        \n",
    "### TO DO (By tomorrow):\n",
    "### Linear activation+backrpop\n",
    "### Make list of individual neurons for easier cache and later analysis ## DONE\n",
    "### Activation function to be checked for each layer n not just for the model. Take this input ## PARTLY DONE\n",
    "### TO DO (By 7 Nov):\n",
    "### Fwd pass and backprop and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796aaa79-1b06-4030-8713-08ddbb60b4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
