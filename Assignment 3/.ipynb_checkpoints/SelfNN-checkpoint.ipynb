{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07412e84-02c6-4388-bbb5-d0cd5ca7f19f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5937564c-d453-48e6-85dd-e9b4f57b2a89",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb9b73-369c-43da-adb5-84182c8ff47e",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c21507-7afe-48a3-a149-b8f1b407417d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#<Something>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c54d48-ec6f-431b-ba64-4108a9cce139",
   "metadata": {},
   "source": [
    "# Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977582a9-2733-4bc8-8b31-f3e846caf558",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e8e037b-1b40-47a2-842e-c8f98ca04aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds_idx(N, nFolds, seed=42):\n",
    "    \"\"\"\n",
    "    Randomly permute [0,N] and extract indices for each fold\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    rnd_idx = np.random.permutation(N)\n",
    "    N_fold = N//nFolds\n",
    "    indices = []\n",
    "    for i in range(nFolds):\n",
    "        start = i*N_fold\n",
    "        end = min([(i+1)*N_fold, N])\n",
    "        # if (N<end):\n",
    "        #     end = N\n",
    "        indices.append(rnd_idx[start:end])\n",
    "    return indices\n",
    "\n",
    "\n",
    "def showImage(img, label):\n",
    "    ficAr = np.array(img, dtype='float')\n",
    "    roughSd = int(math.sqrt(img.size))\n",
    "    pic = picAr.reshape((roughSd, roughSd)).T\n",
    "    # plt.imshow(pix, cmap='gray')\n",
    "    plt.imshow(pix)\n",
    "    lb = str(label)\n",
    "    plt.title('label for this image is',lb)\n",
    "    # print(label)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def act_fn(typ, Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    typ -- sigmoid/RELU\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z)/RELU(z), same shape as Z\n",
    "    cache -- returns Z as well, for backprop\n",
    "    \"\"\"\n",
    "    if (typ.lower()=='sigmoid'):\n",
    "        a = 1/(1+np.exp(-Z))\n",
    "        return a\n",
    "    elif (typ.lower()=='relu'):\n",
    "        a = np.maximum(0,Z)\n",
    "        assert(a.shape == Z.shape)\n",
    "        return a\n",
    "    elif (typ.lower()=='tanh'):\n",
    "        return np.tanh(Z)\n",
    "#     elif (typ.lower()=='softmax'):\n",
    "#         tmp = np.exp(Z)\n",
    "#         a = tmp/np.sum(tmp)\n",
    "#         assert(a.shape == Z.shape)\n",
    "#         return a\n",
    "\n",
    "    \n",
    "def back_fn(typ, Z):\n",
    "    if (typ.lower()=='relu'):\n",
    "        dZ = np.array(Z, copy=True) # Converting dz to a correct object.\n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dZ[Z <= 0] = 0\n",
    "        assert (dZ.shape == Z.shape)\n",
    "        return dZ\n",
    "    elif (typ.lower()=='sigmoid'):\n",
    "        dZ = np.exp(-Z)/(1+np.exp(-Z))**2\n",
    "        assert (dZ.shape == Z.shape)\n",
    "        return dZ\n",
    "    elif (typ.lower()=='tanh'):\n",
    "        dZ = 1-np.tanh(Z)**2\n",
    "        return dZ\n",
    "    # elif (typ.lower()=='softmax'):\n",
    "        \n",
    "    # else return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07395766-6245-4eeb-af13-9822f7806d76",
   "metadata": {},
   "source": [
    "## Class (Cuz I'm fancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822307d-325b-4a36-80a2-8af7a5adc1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_nodes, out_nodes, act_fn):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.act_fun = act_fn\n",
    "        \n",
    "        self.inputs = np.zeros([num_nodes,1]) # output of prev layer\n",
    "        ssz = np.sqrt(num_nodes+out_nodes)\n",
    "        if out_nodes != 0:\n",
    "            self.weights = np.random.randn(num_nodes, out_nodes)/ssz\n",
    "            self.bias = np.random.randn(1, out_nodes)/ssz\n",
    "            # self.outs = np.zeros((1, out_nodes))\n",
    "            # self.ys = np.zeros((out_nodes, 1)) # Linear forward. Intermediate step basically\n",
    "            self.derivs = np.zeros((1, out_nodes))\n",
    "            self.zsList = np.zeros((1, out_nodes))\n",
    "            self.asList = np.zeros((1, out_nodes))\n",
    "        else:\n",
    "            self.weights = None\n",
    "            self.bias = None\n",
    "            # self.outs = None\n",
    "            # self.ys = None\n",
    "            self.derivs = None\n",
    "            self.asList = None\n",
    "            self.zsList = None\n",
    "\n",
    "            \n",
    "            \n",
    "class NeuralNet:\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, act_fns, lr_strat, cost_fn='mse', seed=42):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim # list of dimensions\n",
    "        self.num_layers = len(self.hidden_dim)+2\n",
    "        self.act_fns = act_fns\n",
    "        assert(len(self.act_fns) == self.num_layers-1)\n",
    "        layers = self.hidden_dim.copy()\n",
    "        layers.insert(0, self.input_dim) #Not sure if replacement is inplace\n",
    "        layers.append(self.output_dim)\n",
    "        self.num_nodes = layers #List of number of nodes\n",
    "        self.cost_fn = cost_fn\n",
    "        assert(self.num_layers == len(self.num_nodes))\n",
    "        self.network = []\n",
    "        self.lr_strat = lr_strat\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            if i==self.num_layers-1:\n",
    "                tmp = Layer(num_nodes[i], 0, act_fns[i])\n",
    "            else:\n",
    "                tmp = Layer(num_nodes[i], num_nodes[i+1], act_fns[i])                \n",
    "            self.network.append(tmp)\n",
    "    \n",
    "    def forward(self, xin, target): #Target is just a single number\n",
    "        \n",
    "        self.network[0].inputs = xin #Because first layer can only have input as activation\n",
    "        L=self.num_layers\n",
    "        for i in range(L-1): #Because don't need forward in last layer\n",
    "            self.network[i].asList = self.network[i].bias + np.dot(self.network[i].inputs, self.network[i].weights) # dim = out_nodes x 1\n",
    "            # if ()\n",
    "            self.network[i].zsList = act_fn(self.network[i].act_fns, self.network[i].asList)\n",
    "            self.network[i+1].inputs = self.network[i].zsList\n",
    "        # self.network[L-1].inputs = softmax()\n",
    "        return self.network[L-1].inputs #dim: 1xDim \n",
    "\n",
    "    \n",
    "    def backward(self, y):\n",
    "        L = self.num_layers\n",
    "        y_hat = self.network[L-1].zsList\n",
    "        self.network[L-1].derivs = (y_hat - y) / y.shape[0] # If softmax, then change this ig, idk\n",
    "        for i in range(L-2, -1, -1): #for i in range(N, -1, -1) gives i=N, i=N-1, ..., i=0\n",
    "            err = np.multiply(self.network[i+1].derivs.dot(self.network[i].weights.T), back_fn(self.network[i].act_fns, self.network[i].asList))\n",
    "            self.network[i].derivs = err\n",
    "        # return self.network[0].derivs\n",
    "        return\n",
    "    \n",
    "    def update_weights(self, y, epoch, ilr):\n",
    "        eta = ilr #Initial learning rate\n",
    "        self.backward(y)\n",
    "        L = self.num_layers\n",
    "        for i in range(L-1):\n",
    "            grad_wts = np.dot(self.network[i].zsList.T, self.network[i].derivs)\n",
    "            grad_b = np.sum(self.network[i].derivs, axis=0)\n",
    "            self.network[i].weights -= eta*grad_wts\n",
    "            self.network[i].bias -= eta*grad_b\n",
    "            eta = self.update_lr(ilr, epoch+1)\n",
    "            \n",
    "    def update_lr(self, eta0, iteration):\n",
    "        if (self.lr_strat == 0):\n",
    "            return eta0\n",
    "        else:\n",
    "            return eta0/(iteration**0.5)\n",
    "        \n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "# class NeuralNet:\n",
    "    \n",
    "#     def __init__(self, input_dim=None, output_dim=None, hidden_dim=None, num_hidden_layers=None, typs=None, seed=42):\n",
    "#         # Can add error, no need\n",
    "#         self.input_dim = input_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.num_hid_layers = num_hidden_layers\n",
    "#         assert(len(self.hidden_dim)==self.num_hid_layers)\n",
    "#         self.typs = typs\n",
    "#         self.layers = self.hidden_dim.copy()\n",
    "#         self.layers.insert(0, self.input_dim) #Not sure if replacement is inplace\n",
    "#         self.layers.append(self.output_dim)\n",
    "#         assert(len(self.typs) == self.num_hid_layers+1)\n",
    "#         self.params, self.outs, self.derivs = init_params(self, seed)\n",
    "        \n",
    "#     def init_params(self, seed):\n",
    "#         np.random.seed(seed)\n",
    "#         parameters = {}\n",
    "#         output = {}\n",
    "#         deriv = {}\n",
    "#         # \n",
    "#         L = len(self.layers)\n",
    "#         for l in range(1, L):\n",
    "#             ssz = np.sqrt(self.layers[l]+self.layers[l-1])\n",
    "#             parameters['W' + str(l)] = np.random.randn(self.layers[l],self.layers[l-1])/ssz \n",
    "#             #W(^l)_{i,j} = Weight from lth to l+1th layer. i=destn, j=src idx\n",
    "#             parameters['b' + str(l)] = np.random.randn(self.layers[l],1)/ssz\n",
    "#             output[str(l)] = np.zeros((self.layers[l],1)) # Change to none somehow\n",
    "#             deriv[str(l)] = np.zeros((self.layers[l],1)) # Change to none somehow\n",
    "            \n",
    "#             assert(parameters['W' + str(l)].shape == (self.layers[l], self.layers[l-1]))\n",
    "#             assert(parameters['b' + str(l)].shape == (self.layers[l], 1))\n",
    "#             assert(output[str(l)].shape == (self.layers[l], 1))\n",
    "#             assert(deriv[str(l)].shape == (self.layers[l], 1))\n",
    "\n",
    "#         return parameters, output, deriv\n",
    "    \n",
    "#     def forward(self, xin):\n",
    "#         L = len(self.layers)\n",
    "#         for i in range(1,L):\n",
    "#             xout = []\n",
    "#             self.outs[str(i)] = act_fn(self.typs[i-1], np.dot(self.params['W'+str(i)], xin))\n",
    "#             x_out = self.outs[str(i)]\n",
    "#             xin = x_out # curr output = next input\n",
    "#         yout = cin\n",
    "        \n",
    "#         return yout\n",
    "    \n",
    "#     def backward(self, yout):\n",
    "#         L = len(self.layers)\n",
    "        \n",
    "#         for i in reversed(range(L)): # backwards\n",
    "#             if i == L - 1:\n",
    "#                 # logits/pred - target\n",
    "#                 err = self.outs[str(i)] - yout\n",
    "#                 self.derivs[str(i)] = np.multiply(err, back_fn(self.typs[i-1], self.outs[str(i)]))\n",
    "                \n",
    "#             else:\n",
    "#                 # Weighted sum of derivs\n",
    "#                 err = \n",
    "# #                 for j, node in enumerate(self.network[i]):\n",
    "# #                     err = sum([node_['weights'][j] * node_['delta'] for node_ in self.network[i+1]])\n",
    "# #                     node['delta'] = err * transfer_derivative(node['output'])\n",
    "                    \n",
    "#     def update(self, x, eta):\n",
    "#         for i, layer in enumerate(self.network):\n",
    "#             # Grab input values\n",
    "#             if i == 0: inputs = x\n",
    "#             else: inputs = [node_['output'] for node_ in self.network[i-1]]\n",
    "#             # Update weights\n",
    "#             for node in layer:\n",
    "#                 for j, input in enumerate(inputs):\n",
    "#                     # dw = - learning_rate * (error * transfer') * input\n",
    "#                     node['weights'][j] += - eta * node['delta'] * input\n",
    "    \n",
    "    \n",
    "#     def train(self, X, y, eta=0.5, n_epochs=200):\n",
    "#         for epoch in range(n_epochs):\n",
    "#             for (x_, y_) in zip(X, y):\n",
    "#                 self._forward_pass(x_) # forward pass (update node[\"output\"])\n",
    "#                 yhot_ = self._one_hot_encoding(y_, self.output_dim) # one-hot target\n",
    "#                 self._backward_pass(yhot_) # backward pass error (update node[\"delta\"])\n",
    "#                 self._update_weights(x_, eta) # update weights (update node[\"weight\"])\n",
    "\n",
    "#     # Predict using argmax of logits\n",
    "#     def predict(self, X):\n",
    "#         ypred = np.array([np.argmax(self._forward_pass(x_)) for x_ in X], dtype=np.int)\n",
    "#         return ypred\n",
    "    \n",
    "#     def _one_hot_encoding(self, idx, output_dim):\n",
    "#         x = np.zeros(output_dim, dtype=np.int)\n",
    "#         x[idx] = 1\n",
    "#         return x\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "### TO DO (By tomorrow):\n",
    "### Activation function to be checked for each layer n not just for the model. Take this input ## PARTLY DONE\n",
    "### TO DO (By 7 Nov):\n",
    "### Implement backprop for individual layer. Loop over in training function. Take help from neural_network.ipynb\n",
    "### Fix last layer to be softmax. Softmax just take outputs of last layer and convert them to probability distribution\n",
    "### I have already taken act_fns of size total layers - 1. So softmax can be internally added in the end\n",
    "### Fuck it, Vaibhav ka chhaapo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796aaa79-1b06-4030-8713-08ddbb60b4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
