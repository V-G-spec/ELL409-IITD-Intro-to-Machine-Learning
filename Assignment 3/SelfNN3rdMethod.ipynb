{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07412e84-02c6-4388-bbb5-d0cd5ca7f19f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5937564c-d453-48e6-85dd-e9b4f57b2a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "random_state = 7\n",
    "np.random.seed(random_state)\n",
    "# https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb9b73-369c-43da-adb5-84182c8ff47e",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c21507-7afe-48a3-a149-b8f1b407417d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1    2    3    4    5    6    7    8    9    ...  775  776  777  \\\n",
      "1306  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2037  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "568   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1897  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2498  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "      778  779  780  781  782  783  784  \n",
      "1306  0.0  0.0  0.0  0.0  0.0  0.0  7.0  \n",
      "2037  0.0  0.0  0.0  0.0  0.0  0.0  5.0  \n",
      "568   0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "1897  0.0  0.0  0.0  0.0  0.0  0.0  9.0  \n",
      "2498  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "      0    1    2    3    4    5    6    7    8    9    ...  775  776  777  \\\n",
      "2536  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1452  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "149   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1757  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "218   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "      778  779  780  781  782  783  784  \n",
      "2536  0.0  0.0  0.0  0.0  0.0  0.0  6.0  \n",
      "1452  0.0  0.0  0.0  0.0  0.0  0.0  6.0  \n",
      "149   0.0  0.0  0.0  0.0  0.0  0.0  6.0  \n",
      "1757  0.0  0.0  0.0  0.0  0.0  0.0  8.0  \n",
      "218   0.0  0.0  0.0  0.0  0.0  0.0  6.0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "(2400, 784) (2400, 1) (600, 784) (600, 1)\n"
     ]
    }
   ],
   "source": [
    "#<Something>\n",
    "file_name = '2019EE10143.csv'\n",
    "split_frac = 0.8\n",
    "    \n",
    "def load_data(file_name, split_frac, random_state=random_state):\n",
    "    df = pd.read_csv(file_name, header=None)\n",
    "    cols = len(df.columns) #785\n",
    "    num_ft = cols-1\n",
    "\n",
    "    df = df.sample(frac=1., random_state=random_state)\n",
    "    train_df = df[:int(split_frac*len(df))]\n",
    "    print(train_df.head())\n",
    "    test_df = df[int(split_frac*len(df)):]\n",
    "    print(test_df.head())\n",
    "    X_train_temp = train_df.loc[:, [i for i in range(num_ft)]]\n",
    "    y_train_temp = train_df.loc[:, [num_ft]]\n",
    "    X_test_temp = test_df.loc[:, [i for i in range(num_ft)]]\n",
    "    y_test_temp = test_df.loc[:, [num_ft]]\n",
    "\n",
    "    train_X = np.array(X_train_temp.values)\n",
    "    train_y = np.array(y_train_temp.values)\n",
    "    test_X = np.array(X_test_temp.values)\n",
    "    test_y = np.array(y_test_temp.values)\n",
    "    \n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "train_X, train_y, test_X, test_y = load_data(file_name, split_frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c54d48-ec6f-431b-ba64-4108a9cce139",
   "metadata": {},
   "source": [
    "# Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977582a9-2733-4bc8-8b31-f3e846caf558",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8e037b-1b40-47a2-842e-c8f98ca04aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds_idx(N, nFolds, seed=42):\n",
    "    \"\"\"\n",
    "    Randomly permute [0,N] and extract indices for each fold\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    rnd_idx = np.random.permutation(N)\n",
    "    N_fold = N//nFolds\n",
    "    indices = []\n",
    "    for i in range(nFolds):\n",
    "        start = i*N_fold\n",
    "        end = min([(i+1)*N_fold, N])\n",
    "        # if (N<end):\n",
    "        #     end = N\n",
    "        indices.append(rnd_idx[start:end])\n",
    "    return indices\n",
    "\n",
    "\n",
    "def convToList(y, out_dim):\n",
    "    assert(int(y)==y)\n",
    "    tmp = np.zeros(out_dim, dtype=np.int32)\n",
    "    tmp[int(y)] = 1\n",
    "    return tmp\n",
    "\n",
    "def showImage(img, label):\n",
    "    picAr = np.array(img, dtype='float')\n",
    "    roughSd = int(math.sqrt(img.size))\n",
    "    pic = picAr.reshape((roughSd, roughSd)).T\n",
    "    plt.imshow(pic) #cmap='grey'\n",
    "    lb = str(label)\n",
    "    plt.title('label for this image is',lb)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def act_fn(typ, Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    typ -- sigmoid/RELU\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z)/RELU(z)/tanh(Z), same shape as Z\n",
    "    \"\"\"\n",
    "    if (typ.lower()=='sigmoid'):\n",
    "        a = 1/(1+np.exp(-Z))\n",
    "    elif (typ.lower()=='relu'):\n",
    "        a = np.maximum(0,Z)\n",
    "    elif (typ.lower()=='tanh'):\n",
    "        a= np.tanh(Z)\n",
    "        \n",
    "    assert(a.shape == Z.shape)\n",
    "    return a\n",
    "    \n",
    "def back_fn(typ, Z):\n",
    "    if (typ.lower()=='relu'):\n",
    "        #dZ = np.array(Z, copy=True)\n",
    "        # When z <= 0, set dz to 0. \n",
    "        #dZ[Z <= 0] = 0\n",
    "        dZ = np.array(Z>=0).astype('int')\n",
    "    elif (typ.lower()=='sigmoid'):\n",
    "        dZ = np.exp(-Z)/(1+np.exp(-Z))**2\n",
    "    elif (typ.lower()=='tanh'):\n",
    "        dZ = 1-np.tanh(Z)**2\n",
    "    \n",
    "    assert(dZ.shape==Z.shape)\n",
    "    return dZ\n",
    "\n",
    "    \n",
    "def forCost(typ, logits, yt):\n",
    "    y = convToList(yt, logits.shape[1])\n",
    "    y = np.reshape(y, logits.shape)\n",
    "    #print(y.shape, logits.shape)\n",
    "    #assert(y.shape==logits.shape)\n",
    "    if (typ.lower()=='mse'):\n",
    "        delt = np.power(logits-y,2)\n",
    "        ret= np.mean(delt)\n",
    "        #ret/=y.shape\n",
    "    elif (typ.lower()=='sse'):\n",
    "        delt = np.power(logits-y,2)\n",
    "        ret= np.sum(delt)\n",
    "        ret/=2\n",
    "    elif (typ.lower()=='cross'):\n",
    "        tmp = np.multiply(y, logits) + np.multiply((1-y),(1-logits))\n",
    "        ret = -np.sum(tmp)\n",
    "    else:\n",
    "        print(\"Lmao ded, gonna get an error\")\n",
    "    return ret\n",
    "\n",
    "\n",
    "def backCost(typ, logits, yt):\n",
    "    y = convToList(yt, logits.shape[1])\n",
    "    y = np.reshape(y, logits.shape)\n",
    "    if (typ.lower()=='mse'):\n",
    "        delt=(logits-y)\n",
    "        ret = 2*delt/y.size\n",
    "    elif (typ.lower()=='sse'):\n",
    "        ret = logits-y\n",
    "    elif (typ.lower()=='cross'):\n",
    "        ret = -np.divide(y, logits) + np.divide(1-y, 1-logits)\n",
    "        assert(ret.shape==y.shape)\n",
    "    else:\n",
    "        print(\"Lmao ded, gonna get an error\")\n",
    "    return ret\n",
    "\n",
    "\n",
    "def update_lr(eta0, iteration):\n",
    "#         if (self.lr_strat == 0):\n",
    "#             return eta0\n",
    "#         else:\n",
    "    return eta0/((iteration+1)**0.5)\n",
    "\n",
    "def normalize(X):\n",
    "    return X/255\n",
    "\n",
    "def predict(network, xin):\n",
    "    out=np.reshape(xin, (1, -1))\n",
    "    for layer in network:\n",
    "        out = layer.forward(out)\n",
    "    return out\n",
    "\n",
    "def accuracy(network, test_X, test_y):\n",
    "    corr = 0\n",
    "    for (x,yt) in zip(test_X, test_y):\n",
    "        logits = predict(network, x)\n",
    "        y = convToList(yt, logits.shape[1])\n",
    "        y = np.reshape(y, logits.shape)\n",
    "        if (np.argmax(y) == np.argmax(logits)):\n",
    "            corr+=1\n",
    "    corr/=len(test_y)\n",
    "    return 100*corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07395766-6245-4eeb-af13-9822f7806d76",
   "metadata": {},
   "source": [
    "## Class (Cuz I'm fancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9822307d-325b-4a36-80a2-8af7a5adc1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layers:\n",
    "    # def __init__(self, input_shape=None, input_dim=None, output_dim=None, act_fn=None, typ=None):\n",
    "        \n",
    "    def __init__(self, input_shape=None, input_dim=None, output_dim=None, act_fnc=None, typ=None):\n",
    "        self.typ=typ\n",
    "        if(typ.lower()=='softmax'):\n",
    "            self.input_dim = input_dim\n",
    "        # elif(typ.lower()=='flatten'):\n",
    "        #     self.inshape = input_shape\n",
    "        elif(typ.lower()=='activation'):\n",
    "            self.act_fun = act_fnc\n",
    "        elif(typ.lower()=='fc'): #FC\n",
    "            self.input_dim = input_dim\n",
    "            self.output_dim = output_dim\n",
    "            ssz = np.sqrt((input_dim)/2)\n",
    "            self.weights = np.random.randn(input_dim, output_dim)/ssz\n",
    "            self.bias = np.random.randn(1, output_dim)/ssz\n",
    "        else:\n",
    "            print(\"Oops! Wrong layer type. Gonna go die\")\n",
    "\n",
    "    def forward(self, xin):\n",
    "        if(self.typ.lower()=='softmax'):\n",
    "            self.inputSoft = xin\n",
    "            #exps = np.exp(xin - xin.max())\n",
    "            exps = np.exp(xin)\n",
    "            self.outSoft = exps/np.sum(exps)\n",
    "            return self.outSoft\n",
    "        # elif (self.typ.lower()=='flatten'):\n",
    "        #     return np.reshape(xin, (1, -1))\n",
    "        elif(self.typ.lower()=='activation'):\n",
    "            self.inputAct = xin\n",
    "            return act_fn(self.act_fun, xin)\n",
    "        elif(self.typ.lower()=='fc'): #FC\n",
    "            self.inputFC = xin\n",
    "            return np.dot(xin, self.weights)+self.bias\n",
    "\n",
    "    def backward(self, out_err, lr):\n",
    "        if(self.typ.lower()=='softmax'):\n",
    "            in_err=np.zeros(out_err.shape)\n",
    "            out=np.tile(self.outSoft.T, self.input_dim)\n",
    "            return self.outSoft*np.dot(out_err, np.identity(self.input_dim)-out) ###Can have problems\n",
    "            #tmp = self.inputSoft\n",
    "            #exps = np.exp(tmp-tmp.max())\n",
    "            #tmp2= exps/np.sum(exps)*(1-exps/np.sum(exps))\n",
    "            #return out_err*tmp2\n",
    "        # elif(typ.lower()=='flatten'):\n",
    "        #    return np.reshape(out_err, self.inshape)\n",
    "        elif(self.typ.lower()=='activation'):\n",
    "            return out_err*back_fn(self.act_fun, self.inputAct)\n",
    "        elif(self.typ.lower()=='fc'): #FC\n",
    "            in_err = np.dot(out_err, self.weights.T)\n",
    "#             if(self.inputFC.T.shape[-1] != out_err.shape[0]):\n",
    "#                 print(self.inputFC.T.shape[-1], out_err.shape[0])\n",
    "            assert(self.inputFC.T.shape[-1] == out_err.shape[0])\n",
    "            wt_err = np.dot(self.inputFC.T, out_err)\n",
    "            self.weights-=lr*wt_err\n",
    "            self.bias-=lr*out_err\n",
    "            return in_err\n",
    "\n",
    "    def rettyp(self):\n",
    "        return self.typ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ba9b8-bcfa-4867-8c47-792dc830f6f3",
   "metadata": {},
   "source": [
    "## Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a4aeb0-0bd9-415f-9cf7-d85f1a9253f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_net(typ, num_nodes, activation_fn):\n",
    "    network = []\n",
    "    for idx, ll in enumerate(typ):\n",
    "        if (ll=='fc'):\n",
    "            assert(idx%2==0)\n",
    "            network.append(Layers(typ=ll, input_dim = num_nodes[idx//2], output_dim = num_nodes[(idx//2)+1]))\n",
    "            print(ll, num_nodes[idx//2], num_nodes[(idx//2)+1])\n",
    "        elif (ll=='activation'):\n",
    "            assert((idx-1)%2==0)\n",
    "            network.append(Layers(typ=ll, act_fnc=activation_fn[(idx-1)//2]))\n",
    "            print(ll, activation_fn[(idx-1)//2])\n",
    "        elif (ll=='softmax'):\n",
    "            network.append(Layers(typ=ll, input_dim=num_nodes[-1]))\n",
    "            print(ll, num_nodes[-1])\n",
    "        else:\n",
    "            print(\"You did something wrong there homie. Try again\")\n",
    "    \n",
    "    \n",
    "#     for layer in network:\n",
    "#         print(layer.rettyp())\n",
    "    \n",
    "    return network\n",
    "    \n",
    "### TO DO:\n",
    "### Make function in utilities to create a network, and a function to train\n",
    "### Functions to load data and k-fold cv splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc5c45-713a-4824-9122-14f61a2a4c64",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b81de05-5d22-4dc4-85c7-b8162de8d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, Xtrain, ytrain, epochs=50, initlr=0.1, cost_fn='mse', \n",
    "          early_stop=False, batch_size=30, patience=2, thresh=1e-4, chng_lr=True):\n",
    "    \n",
    "    error = []\n",
    "    checSz=0\n",
    "    lr = initlr\n",
    "    for epoch in range(epochs):\n",
    "        checSz=epoch\n",
    "        if (chng_lr==True):\n",
    "            lr = update_lr(initlr, epoch)\n",
    "        err = 0\n",
    "        for batch in range(0, Xtrain.shape[0], batch_size):\n",
    "            X_batch,y_batch= (Xtrain[batch:batch+batch_size], ytrain[batch:batch+batch_size])\n",
    "            \n",
    "            for (x_b,y_b) in zip(X_batch, y_batch):\n",
    "                out = np.reshape(x_b, (1, -1))\n",
    "                for layer in network:\n",
    "                    out=layer.forward(out)\n",
    "\n",
    "                err+=forCost(cost_fn, out, y_b)\n",
    "                #print(err)\n",
    "                rev_err=backCost(cost_fn, out, y_b)\n",
    "\n",
    "                for layerIdx in range(len(network)):\n",
    "                    layer = network[-1-layerIdx]\n",
    "                    rev_err = layer.backward(rev_err, lr)\n",
    "\n",
    "        err=err/len(Xtrain)\n",
    "        print('%d/%d, Error=%f' % (epoch+1, epochs, err))\n",
    "        error.append(err)\n",
    "        flag=False\n",
    "        if (early_stop==True):\n",
    "            patience=patience+1\n",
    "            if(len(error)>patience):\n",
    "                lastLoss = error[-1]\n",
    "                for i in range(patience):\n",
    "                    if (abs(error[-i-2]-lastLoss)<thresh):\n",
    "                        flag=True\n",
    "                        lastLoss=error[-i-2]\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "        if (flag==True):\n",
    "            break\n",
    "        \n",
    "    #for i in range(EPOCHS):\n",
    "        \n",
    "    assert(len(error)==checSz+1)\n",
    "    if (checSz<epochs-1):\n",
    "        print('Early stopping at %dth epoch'%(checSz+1))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0737d3-9c71-444d-aa8f-83ccedc3bb4e",
   "metadata": {},
   "source": [
    "## User Stuff (Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47b48019-a7b7-457f-bdd4-4c4eab1dae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INPUTS\n",
    "input_shape = (28,28) #I set this depending upon picture\n",
    "\n",
    "prod = 1\n",
    "for i in input_shape:\n",
    "    prod=prod*i\n",
    "\n",
    "num_nodes = [prod, 128, 10] #List of nodes in each layer \n",
    "\n",
    "activation_fn= ['relu'] #List of activation functions for each layer\n",
    "assert(len(activation_fn) == len(num_nodes)-2) #Because last will be softmax\n",
    "\n",
    "typ= ['fc', 'activation', 'fc', 'softmax'] #I set it mp. len = 2*len(act_fn)-2\n",
    "# ALWAYS of the type fc,act,fc,act,...,fc,softmax\n",
    "# DO NOT ADD SOFTMAX ANYWHERE ELSE\n",
    "assert(len(typ) == 2*len(num_nodes)-2)\n",
    "\n",
    "#### TRAINING RELATED HYPERPARAMETERS\n",
    "folds = 5\n",
    "EPOCHS = 10\n",
    "initial_learning_rate = 0.1\n",
    "chng_lr=True\n",
    "cost_fn = 'cross'\n",
    "early_stop = True\n",
    "batch_size=30\n",
    "patience=2\n",
    "thresh=1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89e8ed-baf0-4041-8958-a0947a9089fd",
   "metadata": {},
   "source": [
    "## Let's get Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ed500fb-66cf-45ce-ac24-110b4ed2c3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc 784 128\n",
      "activation relu\n",
      "fc 128 10\n",
      "softmax 10\n",
      "1/10, Error=-8.589823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-24ef3af47c6f>:98: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = -np.divide(y, logits) + np.divide(1-y, 1-logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/10, Error=nan\n",
      "3/10, Error=nan\n",
      "4/10, Error=nan\n",
      "5/10, Error=nan\n",
      "6/10, Error=nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-786bdda981d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#     test_X = normalize(test_X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     error = train(network, X_train, y_train, epochs=EPOCHS, initlr = initial_learning_rate, cost_fn=cost_fn, \n\u001b[0m\u001b[0;32m     29\u001b[0m                   early_stop=early_stop, batch_size=batch_size, patience=patience, thresh=thresh, chng_lr=chng_lr)\n\u001b[0;32m     30\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b47018ab92c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(network, Xtrain, ytrain, epochs, initlr, cost_fn, early_stop, batch_size, patience, thresh, chng_lr)\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                     \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mforCost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-b9c8f275c411>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, xin)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'fc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#FC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputFC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_X\n",
    "# train_y\n",
    "# test_X\n",
    "# test_y\n",
    "\n",
    "N = len(train_X)\n",
    "assert(N==len(train_y))\n",
    "idx_all = np.arange(0, N)\n",
    "idx_folds = get_folds_idx(N, folds, seed=random_state) # list of list of fold indices\n",
    "\n",
    "\n",
    "train_acc = np.array([])\n",
    "dev_acc = np.array([])\n",
    "test_acc = np.array([])\n",
    "# for layer in network:\n",
    "#     print(layer.rettyp())\n",
    "Start = time.process_time()\n",
    "for i,indcs in enumerate(idx_folds):\n",
    "    \n",
    "    network = create_net(typ, num_nodes, activation_fn)\n",
    "    idx_train = np.delete(idx_all, indcs)\n",
    "    X_train, y_train = train_X[idx_train], train_y[idx_train]\n",
    "    X_valid, y_valid = train_X[indcs], train_y[indcs]\n",
    "#     X_train = normalize(X_train)\n",
    "#     X_valid = normalize(X_valid)\n",
    "#     test_X = normalize(test_X)\n",
    "    \n",
    "    error = train(network, X_train, y_train, epochs=EPOCHS, initlr = initial_learning_rate, cost_fn=cost_fn, \n",
    "                  early_stop=early_stop, batch_size=batch_size, patience=patience, thresh=thresh, chng_lr=chng_lr)\n",
    "    train_acc = np.append(train_acc, accuracy(network, X_train, y_train))\n",
    "    dev_acc = np.append(dev_acc, accuracy(network, X_valid, y_valid))\n",
    "    test_acc = np.append(test_acc, accuracy(network, test_X, test_y))\n",
    "#     print('Train accuracy: %f'%())\n",
    "#     print('Dev accuracy: %f'%(accuracy(network, X_valid, y_valid)))\n",
    "#     print('Test accuracy: %f'%(accuracy(network, test_X, test_y)))\n",
    "End = time.process_time()\n",
    "print('Mean train accuracy: %f'%(np.mean(train_acc)))\n",
    "print('Mean dev accuracy: %f'%(np.mean(dev_acc)))\n",
    "print('Mean test accuracy: %f'%(np.mean(test_acc)))\n",
    "print('Total time taken in %dfolds CV: %fseconds'%(folds, End-Start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be073e9-1d30-49f9-b3cd-e632c9fcc387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO DO\n",
    "#k-folds CV is wrong atm, entire model should be retrained, not updated\n",
    "#Automate analysis, and get to studying other courses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
