{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07412e84-02c6-4388-bbb5-d0cd5ca7f19f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5937564c-d453-48e6-85dd-e9b4f57b2a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "random_state = 7\n",
    "np.random.seed(random_state)\n",
    "# https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\n",
    "# https://colab.research.google.com/github/casperbh96/Neural-Network-From-Scratch/blob/master/NN_From_Scratch.ipynb#scrollTo=5IucezTTXiSM\n",
    "# https://github.com/sar-gupta/neural-network-from-scratch/blob/master/neuralnetwork.py\n",
    "# https://hackernoon.com/dl03-gradient-descent-719aff91c7d6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb9b73-369c-43da-adb5-84182c8ff47e",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c21507-7afe-48a3-a149-b8f1b407417d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1    2    3    4    5    6    7    8    9    ...  775  776  777  \\\n",
      "1306  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2037  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "568   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1897  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2498  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "      778  779  780  781  782  783  784  \n",
      "1306  0.0  0.0  0.0  0.0  0.0  0.0  7.0  \n",
      "2037  0.0  0.0  0.0  0.0  0.0  0.0  5.0  \n",
      "568   0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "1897  0.0  0.0  0.0  0.0  0.0  0.0  9.0  \n",
      "2498  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "      0    1    2    3    4    5    6    7    8    9    ...  775  776  777  \\\n",
      "2536  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1452  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "149   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1757  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "218   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "      778  779  780  781  782  783  784  \n",
      "2536  0.0  0.0  0.0  0.0  0.0  0.0  6.0  \n",
      "1452  0.0  0.0  0.0  0.0  0.0  0.0  6.0  \n",
      "149   0.0  0.0  0.0  0.0  0.0  0.0  6.0  \n",
      "1757  0.0  0.0  0.0  0.0  0.0  0.0  8.0  \n",
      "218   0.0  0.0  0.0  0.0  0.0  0.0  6.0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "(2400, 784) (2400, 1) (600, 784) (600, 1)\n"
     ]
    }
   ],
   "source": [
    "#<Something>\n",
    "file_name = '2019EE10143.csv'\n",
    "split_frac = 0.8\n",
    "tot_ex = 1500\n",
    "    \n",
    "def load_data(file_name, split_frac, tot_ex, random_state=random_state):\n",
    "    df = pd.read_csv(file_name, header=None)\n",
    "    cols = len(df.columns) #785\n",
    "    num_ft = cols-1\n",
    "    crop_frac = tot_ex/len(df)\n",
    "    df = df.sample(frac=crop_frac, random_state=random_state)\n",
    "    train_df = df[:int(split_frac*len(df))]\n",
    "    print(train_df.head())\n",
    "    test_df = df[int(split_frac*len(df)):]\n",
    "    print(test_df.head())\n",
    "    X_train_temp = train_df.loc[:, [i for i in range(num_ft)]]\n",
    "    y_train_temp = train_df.loc[:, [num_ft]]\n",
    "    X_test_temp = test_df.loc[:, [i for i in range(num_ft)]]\n",
    "    y_test_temp = test_df.loc[:, [num_ft]]\n",
    "\n",
    "    train_X = np.array(X_train_temp.values)\n",
    "    train_y = np.array(y_train_temp.values)\n",
    "    test_X = np.array(X_test_temp.values)\n",
    "    test_y = np.array(y_test_temp.values)\n",
    "    \n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "train_X, train_y, test_X, test_y = load_data(file_name, split_frac, tot_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977582a9-2733-4bc8-8b31-f3e846caf558",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8e037b-1b40-47a2-842e-c8f98ca04aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds_idx(N, nFolds, seed=42):\n",
    "    \"\"\"\n",
    "    Randomly permute [0,N] and extract indices for each fold\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    rnd_idx = np.random.permutation(N)\n",
    "    N_fold = N//nFolds\n",
    "    indices = []\n",
    "    for i in range(nFolds):\n",
    "        start = i*N_fold\n",
    "        end = min([(i+1)*N_fold, N])\n",
    "        # if (N<end):\n",
    "        #     end = N\n",
    "        indices.append(rnd_idx[start:end])\n",
    "    return indices\n",
    "\n",
    "\n",
    "def convToList(y, out_dim):\n",
    "    assert(int(y)==y)\n",
    "    tmp = np.zeros(out_dim, dtype=np.int32)\n",
    "    tmp[int(y)] = 1\n",
    "    return tmp\n",
    "\n",
    "def showImage(img, label):\n",
    "    picAr = np.array(img, dtype='float')\n",
    "    roughSd = int(math.sqrt(img.size))\n",
    "    pic = picAr.reshape((roughSd, roughSd)).T\n",
    "    plt.imshow(pic) #cmap='grey'\n",
    "    lb = str(label)\n",
    "    plt.title('label for this image is',lb)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def act_fn(typ, Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    typ -- sigmoid/RELU\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z)/RELU(z)/tanh(Z), same shape as Z\n",
    "    \"\"\"\n",
    "    if (typ.lower()=='sigmoid'):\n",
    "        a = 1/(1+np.exp(-Z))\n",
    "    elif (typ.lower()=='relu'):\n",
    "        a = np.maximum(0,Z)\n",
    "    elif (typ.lower()=='tanh'):\n",
    "        a= np.tanh(Z)\n",
    "        \n",
    "    assert(a.shape == Z.shape)\n",
    "    return a\n",
    "    \n",
    "def back_fn(typ, Z):\n",
    "    if (typ.lower()=='relu'):\n",
    "        #dZ = np.array(Z, copy=True)\n",
    "        # When z <= 0, set dz to 0. \n",
    "        #dZ[Z <= 0] = 0\n",
    "        dZ = np.array(Z>=0).astype('int')\n",
    "    elif (typ.lower()=='sigmoid'):\n",
    "        dZ = np.exp(-Z)/(1+np.exp(-Z))**2\n",
    "    elif (typ.lower()=='tanh'):\n",
    "        dZ = 1-np.tanh(Z)**2\n",
    "    \n",
    "    assert(dZ.shape==Z.shape)\n",
    "    return dZ\n",
    "\n",
    "    \n",
    "def forCost(typ, logits, yt):\n",
    "    y = convToList(yt, logits.shape[1])\n",
    "    y = np.reshape(y, logits.shape)\n",
    "    if (typ.lower()=='mse'):\n",
    "        delt = np.power(logits-y,2)\n",
    "        ret= np.mean(delt)\n",
    "        #ret/=y.shape\n",
    "    elif (typ.lower()=='sse'):\n",
    "        delt = np.power(logits-y,2)\n",
    "        ret= np.sum(delt)\n",
    "        ret/=2\n",
    "    elif (typ.lower()=='cross'):\n",
    "        tmp = np.multiply(y, logits) + np.multiply((1-y),(1-logits))\n",
    "        ret = -np.sum(tmp)\n",
    "    else:\n",
    "        print(\"Lmao ded, gonna get an error\")\n",
    "    return ret\n",
    "\n",
    "\n",
    "def backCost(typ, logits, yt):\n",
    "    y = convToList(yt, logits.shape[1])\n",
    "    y = np.reshape(y, logits.shape)\n",
    "    if (typ.lower()=='mse'):\n",
    "        delt=(logits-y)\n",
    "        ret = 2*delt/y.size\n",
    "    elif (typ.lower()=='sse'):\n",
    "        ret = logits-y\n",
    "    elif (typ.lower()=='cross'):\n",
    "        ret = -np.divide(y, logits) + np.divide(1-y, 1-logits)\n",
    "        assert(ret.shape==y.shape)\n",
    "    else:\n",
    "        print(\"Lmao ded, gonna get an error\")\n",
    "    return ret\n",
    "\n",
    "\n",
    "def update_lr(eta0, iteration):\n",
    "    return eta0/((iteration+1)**0.5)\n",
    "\n",
    "def normalize(X):\n",
    "    return X/255\n",
    "\n",
    "def predict(network, xin):\n",
    "    out=np.reshape(xin, (1, -1))\n",
    "    for layer in network:\n",
    "        out = layer.forward(out)\n",
    "    return out\n",
    "\n",
    "def accuracy(network, test_X, test_y):\n",
    "    corr = 0\n",
    "    for (x,yt) in zip(test_X, test_y):\n",
    "        logits = predict(network, x)\n",
    "        y = convToList(yt, logits.shape[1])\n",
    "        y = np.reshape(y, logits.shape)\n",
    "        if (np.argmax(y) == np.argmax(logits)):\n",
    "            corr+=1\n",
    "    corr/=len(test_y)\n",
    "    return 100*corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07395766-6245-4eeb-af13-9822f7806d76",
   "metadata": {},
   "source": [
    "## Class (Cuz I'm fancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9822307d-325b-4a36-80a2-8af7a5adc1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layers:\n",
    "    # def __init__(self, input_shape=None, input_dim=None, output_dim=None, act_fn=None, typ=None):\n",
    "        \n",
    "    def __init__(self, input_shape=None, input_dim=None, output_dim=None, act_fnc=None, typ=None):\n",
    "        self.typ=typ\n",
    "        if(typ.lower()=='softmax'):\n",
    "            self.input_dim = input_dim\n",
    "        elif(typ.lower()=='activation'):\n",
    "            self.act_fun = act_fnc\n",
    "        elif(typ.lower()=='fc'): #FC\n",
    "            self.input_dim = input_dim\n",
    "            self.output_dim = output_dim\n",
    "            ssz = np.sqrt((input_dim)/2)\n",
    "            self.weights = np.random.randn(input_dim, output_dim)/ssz\n",
    "            self.bias = np.random.randn(1, output_dim)/ssz\n",
    "        else:\n",
    "            print(\"Oops! Wrong layer type. Gonna go die\")\n",
    "\n",
    "    def forward(self, xin):\n",
    "        if(self.typ.lower()=='softmax'):\n",
    "            self.inputSoft = xin\n",
    "            #exps = np.exp(xin - xin.max())\n",
    "            exps = np.exp(xin)\n",
    "            self.outSoft = exps/np.sum(exps)\n",
    "            return self.outSoft\n",
    "        elif(self.typ.lower()=='activation'):\n",
    "            self.inputAct = xin\n",
    "            return act_fn(self.act_fun, xin)\n",
    "        elif(self.typ.lower()=='fc'): #FC\n",
    "            self.inputFC = xin\n",
    "            return np.dot(xin, self.weights)+self.bias\n",
    "\n",
    "    def backward(self, out_err, lr):\n",
    "        if(self.typ.lower()=='softmax'):\n",
    "            in_err=np.zeros(out_err.shape)\n",
    "            out=np.tile(self.outSoft.T, self.input_dim)\n",
    "            return self.outSoft*np.dot(out_err, np.identity(self.input_dim)-out) ###Can have problems\n",
    "            #tmp = self.inputSoft\n",
    "            #exps = np.exp(tmp-tmp.max())\n",
    "            #tmp2= exps/np.sum(exps)*(1-exps/np.sum(exps))\n",
    "            #return out_err*tmp2\n",
    "        elif(self.typ.lower()=='activation'):\n",
    "            return out_err*back_fn(self.act_fun, self.inputAct)\n",
    "        elif(self.typ.lower()=='fc'): #FC\n",
    "            in_err = np.dot(out_err, self.weights.T)\n",
    "            assert(self.inputFC.T.shape[-1] == out_err.shape[0])\n",
    "            wt_err = np.dot(self.inputFC.T, out_err)\n",
    "            self.weights-=lr*wt_err\n",
    "            self.bias-=lr*out_err\n",
    "            return in_err\n",
    "\n",
    "    def rettyp(self):\n",
    "        return self.typ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ba9b8-bcfa-4867-8c47-792dc830f6f3",
   "metadata": {},
   "source": [
    "## Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a4aeb0-0bd9-415f-9cf7-d85f1a9253f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_net(typ, num_nodes, activation_fn, tm):\n",
    "    network = []\n",
    "    for idx, ll in enumerate(typ):\n",
    "        if (ll=='fc'):\n",
    "            assert(idx%2==0)\n",
    "            network.append(Layers(typ=ll, input_dim = num_nodes[idx//2], output_dim = num_nodes[(idx//2)+1]))\n",
    "            if (tm==0):\n",
    "                print('Layer type: %s \\nInput Dimension: %d \\t Output Dimension: %d'%(ll.title(),num_nodes[idx//2],num_nodes[(idx//2)+1]))\n",
    "        elif (ll=='activation'):\n",
    "            assert((idx-1)%2==0)\n",
    "            network.append(Layers(typ=ll, act_fnc=activation_fn[(idx-1)//2]))\n",
    "            if (tm==0):\n",
    "                print('Layer type: %s \\nActivation Function: %s'%(ll.title(), activation_fn[(idx-1)//2].title()))\n",
    "        elif (ll=='softmax'):\n",
    "            network.append(Layers(typ=ll, input_dim=num_nodes[-1]))\n",
    "            if (tm==0):\n",
    "                print('Layer type: %s \\nInput Dimension: %d'%(ll.title(), num_nodes[-1]))\n",
    "        else:\n",
    "            print(\"You did something wrong there homie. Try again\")\n",
    "    \n",
    "    \n",
    "#     for layer in network:\n",
    "#         print(layer.rettyp())\n",
    "    \n",
    "    return network\n",
    "    \n",
    "### TO DO:\n",
    "### Make function in utilities to create a network, and a function to train\n",
    "### Functions to load data and k-fold cv splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc5c45-713a-4824-9122-14f61a2a4c64",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b81de05-5d22-4dc4-85c7-b8162de8d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, Xtrain, ytrain, epochs=50, initlr=0.1, cost_fn='mse', \n",
    "          early_stop=False, batch_size=30, patience=2, thresh=1e-4, chng_lr=True):\n",
    "    \n",
    "    error = []\n",
    "    checSz=0\n",
    "    lr = initlr\n",
    "    for epoch in range(epochs):\n",
    "        checSz=epoch\n",
    "        if (chng_lr==True):\n",
    "            lr = update_lr(initlr, epoch)\n",
    "        err = 0\n",
    "        for batch in range(0, Xtrain.shape[0], batch_size):\n",
    "            X_batch,y_batch= (Xtrain[batch:batch+batch_size], ytrain[batch:batch+batch_size])\n",
    "            \n",
    "            for (x_b,y_b) in zip(X_batch, y_batch):\n",
    "                out = np.reshape(x_b, (1, -1))\n",
    "                for layer in network:\n",
    "                    out=layer.forward(out)\n",
    "\n",
    "                err+=forCost(cost_fn, out, y_b)\n",
    "                #print(err)\n",
    "                rev_err=backCost(cost_fn, out, y_b)\n",
    "\n",
    "                for layerIdx in range(len(network)):\n",
    "                    layer = network[-1-layerIdx]\n",
    "                    rev_err = layer.backward(rev_err, lr)\n",
    "\n",
    "        err=err/len(Xtrain)\n",
    "        #print('%d/%d, Error=%f' % (epoch+1, epochs, err))\n",
    "        error.append(err)\n",
    "        flag=False\n",
    "        if (early_stop==True):\n",
    "            patience=patience+1\n",
    "            if(len(error)>patience):\n",
    "                lastLoss = error[-1]\n",
    "                for i in range(patience):\n",
    "                    if (abs(error[-i-2]-lastLoss)<thresh):\n",
    "                        flag=True\n",
    "                        lastLoss=error[-i-2]\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "        if (flag==True):\n",
    "            break\n",
    "        \n",
    "    #for i in range(EPOCHS):\n",
    "        \n",
    "    assert(len(error)==checSz+1)\n",
    "    if (checSz<epochs-1):\n",
    "        print('Early stopping at %dth epoch'%(checSz+1))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0737d3-9c71-444d-aa8f-83ccedc3bb4e",
   "metadata": {},
   "source": [
    "## User Stuff (Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47b48019-a7b7-457f-bdd4-4c4eab1dae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INPUTS\n",
    "input_shape = (28,28) #I set this depending upon picture\n",
    "\n",
    "prod = 1\n",
    "for i in input_shape:\n",
    "    prod=prod*i\n",
    "\n",
    "num_nodes = [prod, 128, 10] #List of nodes in each layer \n",
    "\n",
    "activation_fn= ['relu'] #List of activation functions for each layer\n",
    "assert(len(activation_fn) == len(num_nodes)-2) #Because last will be softmax\n",
    "\n",
    "typ= ['fc', 'activation', 'fc', 'softmax'] #I set it mp. len = 2*len(act_fn)-2\n",
    "# ALWAYS of the type fc,act,fc,act,...,fc,softmax\n",
    "# DO NOT ADD SOFTMAX ANYWHERE ELSE\n",
    "assert(len(typ) == 2*len(num_nodes)-2)\n",
    "\n",
    "#### TRAINING RELATED HYPERPARAMETERS\n",
    "folds = 5\n",
    "EPOCHS = 10\n",
    "initial_learning_rate = 0.1\n",
    "chng_lr=True\n",
    "cost_fn = 'mse'\n",
    "early_stop = True\n",
    "batch_size=30\n",
    "patience=2\n",
    "thresh=1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718af79-2676-4fd2-a1cf-3c285f4998e7",
   "metadata": {},
   "source": [
    "# MEGA PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ad864b-95fd-49b0-8931-30ebbcdbd7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_epochs = [1, 10, 20, 50]\n",
    "diff_lr = [0.001, 0.01, 0.1, 1]\n",
    "diff_costs = ['mse', 'sse']\n",
    "diff_chng_lr= [True, False]\n",
    "diff_early_stop = [True, False]\n",
    "diff_batch_size= [1, 10, 20, 50, 100]\n",
    "\n",
    "N = len(train_X)\n",
    "assert(N==len(train_y))\n",
    "idx_all = np.arange(0, N)\n",
    "idx_folds = get_folds_idx(N, folds, seed=random_state) # list of list of fold indices\n",
    "\n",
    "tot_mods = 0\n",
    "for EPOCHS in diff_epochs:\n",
    "    for initial_learning_rate in diff_lr:\n",
    "        for cost_fn in diff_costs:\n",
    "            for chng_lr in diff_chng_lr:\n",
    "                for early_stop in diff_early_stop:\n",
    "                    for batch_size in diff_batch_size:\n",
    "                        tot_mods+=1\n",
    "                        print()\n",
    "                        print('--------------------------------------------------')\n",
    "                        print('Iteration Number:', tot_mods)\n",
    "                        print('Number of Epochs: %d \\nCost Function used: %s \\nLearning Rate is being updated?: %s \\nEarly Stopping Regularization being used?: %s \\nBatch Size: %d'%(EPOCHS, \n",
    "                                  cost_fn.title(), 'Yes' if chng_lr==True else 'No', 'Yes' if early_stop==True else 'No', batch_size))\n",
    "\n",
    "                        train_acc = np.array([])\n",
    "                        dev_acc = np.array([])\n",
    "                        test_acc = np.array([])\n",
    "\n",
    "                        Start = time.process_time()\n",
    "                        for i,indcs in enumerate(idx_folds):\n",
    "                            print(\"Fold Number: %d/%d\"%(i+1, len(idx_folds)))\n",
    "                            network = create_net(typ, num_nodes, activation_fn, i)\n",
    "                            idx_train = np.delete(idx_all, indcs)\n",
    "                            X_train, y_train = train_X[idx_train], train_y[idx_train]\n",
    "                            X_valid, y_valid = train_X[indcs], train_y[indcs]\n",
    "                            if (i==0):\n",
    "                                print(\"Training Size:\", X_train.shape)\n",
    "                                print(\"Validation Size:\", X_valid.shape)\n",
    "                            try:\n",
    "                                error = train(network, X_train, y_train, epochs=EPOCHS, initlr = initial_learning_rate, cost_fn=cost_fn, \n",
    "                                            early_stop=early_stop, batch_size=batch_size, patience=patience, thresh=thresh, chng_lr=chng_lr)\n",
    "                                print('Initial loss:%f | Final loss:%f'%(error[0], error[-1]))\n",
    "                                train_acc = np.append(train_acc, accuracy(network, X_train, y_train))\n",
    "                                dev_acc = np.append(dev_acc, accuracy(network, X_valid, y_valid))\n",
    "                                #test_acc = np.append(test_acc, accuracy(network, test_X, test_y))\n",
    "                            except:\n",
    "                                print(\"Some issue with this iteration... Moving on to the next one!\")\n",
    "                        End = time.process_time()\n",
    "                        print('Mean train accuracy: %f'%(np.mean(train_acc)))\n",
    "                        print('Mean dev accuracy: %f'%(np.mean(dev_acc)))\n",
    "                        #print('Mean test accuracy: %f'%(np.mean(test_acc)))\n",
    "                        print('Total time taken in %d-folds CV on given set of hyperparameters: %f seconds'%(folds, End-Start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471851d-e3e4-4e2a-8127-0045779227cb",
   "metadata": {},
   "source": [
    "## Run Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb99bb5-0d96-48a2-9406-a558edc84c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(train_X)\n",
    "assert(N==len(train_y))\n",
    "idx_all = np.arange(0, N)\n",
    "idx_folds = get_folds_idx(N, folds, seed=random_state) # list of list of fold indices\n",
    "\n",
    "\n",
    "train_acc = np.array([])\n",
    "dev_acc = np.array([])\n",
    "test_acc = np.array([])\n",
    "\n",
    "Start = time.process_time()\n",
    "for i,indcs in enumerate(idx_folds):\n",
    "    \n",
    "    network = create_net(typ, num_nodes, activation_fn)\n",
    "    idx_train = np.delete(idx_all, indcs)\n",
    "    X_train, y_train = train_X[idx_train], train_y[idx_train]\n",
    "    X_valid, y_valid = train_X[indcs], train_y[indcs]\n",
    "\n",
    "    error = train(network, X_train, y_train, epochs=EPOCHS, initlr = initial_learning_rate, cost_fn=cost_fn, \n",
    "                  early_stop=early_stop, batch_size=batch_size, patience=patience, thresh=thresh, chng_lr=chng_lr)\n",
    "    print('Initial loss:%f | Final loss:%f'%(error[0], error[-1]))\n",
    "    train_acc = np.append(train_acc, accuracy(network, X_train, y_train))\n",
    "    dev_acc = np.append(dev_acc, accuracy(network, X_valid, y_valid))\n",
    "    test_acc = np.append(test_acc, accuracy(network, test_X, test_y))\n",
    "\n",
    "End = time.process_time()\n",
    "print('Mean train accuracy: %f'%(np.mean(train_acc)))\n",
    "print('Mean dev accuracy: %f'%(np.mean(dev_acc)))\n",
    "print('Mean test accuracy: %f'%(np.mean(test_acc)))\n",
    "print('Total time taken in %d-folds CV: %f seconds'%(folds, End-Start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bedacd-8799-4684-9953-5378029a1a76",
   "metadata": {},
   "source": [
    "## Iterate over single hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96abf5fe-b9e0-4d48-aa85-ea8801a0b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants = {'NumEpochs': 20, 'InitialLr': 0.001, 'BatchSize': 1}\n",
    "# vars_to_differ = [diff_epochs, diff_lr, diff_lr_lin, diff_batch_size]\n",
    "\n",
    "diff_epochs = [1, 5, 10, 20, 50]\n",
    "diff_lr = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "diff_lr_lin = [0.001, 0.002, 0.004, 0.005, 0.007, 0.008]\n",
    "diff_batch_size= [1, 10, 20, 50, 100, 200, 500]\n",
    "\n",
    "N = len(train_X)\n",
    "assert(N==len(train_y))\n",
    "idx_all = np.arange(0, N)\n",
    "idx_folds = get_folds_idx(N, folds, seed=random_state) # list of list of fold indices\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "initial_learning_rate = 0.001\n",
    "cost_fn = 'sse'\n",
    "chng_lr = True\n",
    "early_stop = True\n",
    "batch_size = 1\n",
    "\n",
    "mean_train = np.array([])\n",
    "std_train = np.array([])\n",
    "mean_test = np.array([])\n",
    "std_test = np.array([])\n",
    "time_taken = np.array([])\n",
    "print()\n",
    "print('--------------------------------------------------')\n",
    "print(\"Curves for epochs\")\n",
    "\n",
    "for EPOCHS in diff_epochs:\n",
    "  \n",
    "    train_acc = np.array([])\n",
    "    test_acc = np.array([])\n",
    "\n",
    "    Start = time.process_time()\n",
    "    for i,indcs in enumerate(idx_folds):\n",
    "        network = create_net(typ, num_nodes, activation_fn, 1)\n",
    "        idx_train = np.delete(idx_all, indcs)\n",
    "        X_train, y_train = train_X[idx_train], train_y[idx_train]\n",
    "        X_valid, y_valid = train_X[indcs], train_y[indcs]\n",
    "\n",
    "        error = train(network, X_train, y_train, epochs=EPOCHS, initlr = initial_learning_rate, cost_fn=cost_fn, \n",
    "                  early_stop=early_stop, batch_size=batch_size, patience=patience, thresh=thresh, chng_lr=chng_lr)\n",
    "        train_acc = np.append(train_acc, accuracy(network, X_train, y_train))\n",
    "        test_acc = np.append(test_acc, accuracy(network, test_X, test_y))\n",
    "        \n",
    "    End = time.process_time()\n",
    "\n",
    "    time_taken = np.append(time_taken, End-Start)\n",
    "    mean_train=np.append(mean_train, np.mean(train_acc))\n",
    "    mean_test= np.append(mean_test, np.mean(test_acc))\n",
    "    std_train= np.append(std_train, np.std(train_acc))\n",
    "    std_test= np.append(std_test, np.std(test_acc))\n",
    "  \n",
    "  \n",
    "\n",
    "plt.errorbar(diff_epochs, mean_train, std_train, linestyle='--', marker='o', label='train')\n",
    "plt.errorbar(diff_epochs, mean_test, std_test, linestyle='-', marker='^',label='test' )\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Accuracy Vs Number of epochs')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(diff_epochs, time_taken)\n",
    "plt.title('Time vs Number of epochs')\n",
    "plt.ylabel('Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
